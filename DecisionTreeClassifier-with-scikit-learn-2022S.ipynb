{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier with scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Creative Commons License](https://i.creativecommons.org/l/by/4.0/88x31.png)  \n",
    "This work by Jephian Lin is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(<parameters>)\n",
    "model.fit(X, y)\n",
    "y_new = model.predict(X_test)\n",
    "```\n",
    "\n",
    "[Official Reference](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "- `criterion`: `\"gini\"` or `\"entropy\"`  \n",
    "the function to measure a good cut\n",
    "- `max_depth`: an integer, the maximum depth of a tree\n",
    "- `min_samples_split`: if a node has more than `min_samples_split` samples, then split further\n",
    "\n",
    "and many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes\n",
    "- `classes_`: an array of shape `(n_classes,)`  \n",
    "(Usually `0, ..., n_classes-1`)\n",
    "- `feature_importances_`: an array of shape `(n_features,)`  \n",
    "the total importance (impurity reduction) of each feature\n",
    "- `tree_`: the constructed decition tree\n",
    "\n",
    "For `model.tree_`  \n",
    "- `children_left[i]`: id of the left child of node i or -1 if leaf node\n",
    "- `children_right[i]`: id of the right child of node i or -1 if leaf node\n",
    "- `feature[i]`: feature used for splitting node i\n",
    "- `threshold[i]`: threshold value at node i\n",
    "- `n_node_samples[i]`: the number of of training samples reaching node i\n",
    "- `impurity[i]`: the impurity at node i\n",
    "\n",
    "(Source: [Understanding the decision tree structure](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 1\n",
    "Let  \n",
    "```python\n",
    "mu = np.array([1,1])\n",
    "cov = np.array([[1.1,-1],\n",
    "                [-1,1.1]])\n",
    "X = np.vstack([np.random.multivariate_normal(mu, cov, 100), \n",
    "               np.random.multivariate_normal(-mu, cov, 100)])\n",
    "y = np.array([0]*100 + [1]*100)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1(a)\n",
    "Plot the points (rows) in `X` with `c=y` .  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "mu = np.array([1,1])\n",
    "cov = np.array([[1.1,-1],\n",
    "                [-1,1.1]])\n",
    "X = np.vstack([np.random.multivariate_normal(mu, cov, 100), \n",
    "               np.random.multivariate_normal(-mu, cov, 100)])\n",
    "y = np.array([0]*100 + [1]*100)\n",
    "\n",
    "plt.scatter(*X.T, c=y)   ##先畫原本的 X,y 的資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1(b)\n",
    "Draw 1000 random points uniformly on the region $-5\\leq x\\leq 5$ and $-5\\leq y\\leq 5$.  \n",
    "Use the trained model to make a prediction `y_new` .  \n",
    "Then plot these 1000 points with `c=y_new` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "#建立模型\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "X_test = np.random.rand(1000,2) * 10 - np.array([5,5])  ##創建測試集的資料\n",
    "y_new = model.predict(X_test)\n",
    "plt.scatter(*X.T, c=y)  ##深色為原本的資料\n",
    "plt.scatter(*X_test.T, c=y_new, s=10, alpha=0.3)  ## 依照分類樹模型來預測那測試集的點，結果為較淺較小點的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1(c)\n",
    "Trains a decision tree model.  \n",
    "Let  \n",
    "```python\n",
    "from sklearn.tree import plot_tree\n",
    "plot_tree(model)\n",
    "```\n",
    "Try to understand the following questions:\n",
    "- Check if the number of samples in a node is equal to the sum of those in its two children\n",
    "- What can you say about the `gini` value at each leaf node?\n",
    "- What can you say about the `value` distribution at each leaf node?\n",
    "- Check how many samples satisfies the criteria at the root node.  It should be the same as the number of samples in the left child (of the root)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "from sklearn.tree import plot_tree\n",
    "plot_tree(model,class_names=['a','b'])\n",
    "# \n",
    "# ANS:\n",
    "# 1. Yes.\n",
    "#    The number of samples in a node is equal to the sum of those in its two children\n",
    "#    (ex: 110 + 90 = 200, 93 + 17 = 110, ...)\n",
    "# 2. Gini:\n",
    "#    gini is the degree of impurity of the dataset.\n",
    "#    For example, there are 200 data points in the original dataset.At the first classification,\n",
    "#    the model classify it into 100 points which labeled by \"a\", and 100 points labeled as \"b\".\n",
    "#    100 points are correct, and 100 are mislabeled, so the gini = 0.5(1-0.5) + 0.5(1-0.5) = 0.5\n",
    "#    let's take a look at the another example, we focus on left child of the second level,\n",
    "#    therr are 81 samples , the model classify it into 5 points which labeled by \"a\", and 76 points labeled as \"b\".\n",
    "#    76 points are correct, and 5 are mislabeled, so the gini = 17/110(1 - 17/110) + 93/110(1 - 93/110) = 0.261\n",
    "#    As we see, after few time of classification, gini will decrease to 0,\n",
    "#    only when the gini=0 , the model will stop classifying, because gini=0 means that the node is pure.\n",
    "\n",
    "# 3. Value:\n",
    "#    value means the number of data points in each class,\n",
    "#    in this case, it means that there are how many points whick are labeled by \"a\" or \"b\".\n",
    "#    Take the left child at the second level as example, value=[17,93] means that\n",
    "#    17 points should be labeled as a, but they are mislabeles as b.\n",
    "#    Only if value=[0, xx] or [xx, 0] will the model stop classifying,\n",
    "#    because it means that there are no points are mislabeled.\n",
    "\n",
    "# 4. There are 110 points satisfies the criteria at the root node.\n",
    "j = model.tree_.feature[0]\n",
    "t = model.tree_.threshold[0]\n",
    "### x[j] <= t\n",
    "(X[:,j] < t).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 2\n",
    "Let  \n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2(a)\n",
    "Apply the decision tree classification algorithm to `X` and `y` .  \n",
    "Make a predicion of the training data.  \n",
    "How is the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "import sklearn.model_selection\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size = 0.2)\n",
    "# 80% training and 20% test\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_new = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy = \",metrics.accuracy_score(y_test, y_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2(b)\n",
    "Plot the decision tree.  \n",
    "If a bag of $n$ balls contains $n_i$ balls of color $i$ (for colors $i=0,\\ldots,c-1$), the **Gini impurity** of this bag (distribution) is  \n",
    "$$\\sum_{i=0}^{c-1} p_i(1 - p_i),$$\n",
    "where $p_i = n_i/n$ is the probability of getting a ball of color $i$.\n",
    "\n",
    "Pick a node and check if this formula is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize = (12,12))\n",
    "plot_tree(model)\n",
    "(39/120)*(1-39/120) + (37/120)*(1-37/120) + (44/120)*(1-44/120)\n",
    "## the answer = 0.66486, is close to the gini = 0.665 in the top node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2(c)\n",
    "Change the model setting to `criteria=\"entropy\"` and plot the decision tree again.  \n",
    "If a bag of $n$ balls contains $n_i$ balls of color $i$ (for colors $i=0,\\ldots,c-1$), the **entropy** of this bag (distribution) is  \n",
    "$$\\sum_{i=0}^{c-1} -p_i\\log_2(p_i),$$\n",
    "where $p_i = n_i/n$ is the probability of getting a ball of color $i$.\n",
    "\n",
    "Pick a node and check if this formula is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "import math\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "model.fit(X_train, y_train)\n",
    "y_new = model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize = (12,12))\n",
    "plot_tree(model)\n",
    "\n",
    "-(39/120)*np.log2(39/120) - (37/120)*np.log2(37/120) - (44/120)*np.log2(44/120)\n",
    "## the answer= 1.58109,it is close to the entropy = 1.581 in the top node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3\n",
    "Let  \n",
    "```python\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "mask = (digits.target == 0) | (digits.target == 1)\n",
    "X = digits.data[mask]\n",
    "y = digits.target[mask]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3(a)\n",
    "Train a decision tree classification model.  \n",
    "How is its accuracy score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import metrics\n",
    "\n",
    "digits = load_digits()\n",
    "mask = (digits.target == 0) | (digits.target == 1)\n",
    "X = digits.data[mask]\n",
    "y = digits.target[mask]\n",
    "\n",
    "digits = load_digits()\n",
    "mask = (digits.target == 0) | (digits.target == 1)\n",
    "X = digits.data[mask]\n",
    "y = digits.target[mask]\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "y_new = model.predict(X)\n",
    "print(\"Testing data accuracy:\",metrics.accuracy_score(y, y_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3(b)\n",
    "Use any software or online app to draw a picture of 0 or 1.  \n",
    "Save it as a file, e.g., `my_digit.png` .  \n",
    "Use the following code to load it.  \n",
    "```python\n",
    "from PIL import Image\n",
    "img = Image.open(\"my_digit.png\").resize(8,8)\n",
    "```\n",
    "Does the model give you the right answer?  \n",
    "Each of you can do 5 pictures.  \n",
    "Let's see what is the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "import os \n",
    "path = \"DecisionTreeClassifier-with-scikit-learn-2021S-data\"\n",
    "\n",
    "### your answer here\n",
    "from PIL import Image\n",
    "def ImageToMatrix(filename):\n",
    "    # 读取图片\n",
    "    im = Image.open(filename).resize((8,8))\n",
    " \n",
    "    width,height = im.size\n",
    "    im = im.convert(\"L\") \n",
    "    data = im.getdata()\n",
    "    data = np.array(data,dtype='float')/255.0\n",
    "    #new_data = np.reshape(data,(width,height))\n",
    "    new_data = np.reshape(data,(height,width))\n",
    "    return new_data\n",
    "\n",
    "filename = os.path.join(path, \"my_digit.png\") \n",
    "data = ImageToMatrix(filename) \n",
    "# data = ImageToMatrix(\"my_digit.png\")\n",
    "\n",
    "data[data < 1] = 0\n",
    "\n",
    "mask0 = (data[:,6] == 0)\n",
    "mask1 = (data[:,6] == 1)\n",
    "X = data[:,mask0]\n",
    "y = data[:,mask1]\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "y_new = model.predict(X)\n",
    "print(\"Testing data accuracy:\",metrics.accuracy_score(y, y_new))\n",
    "\n",
    "\n",
    "def MatrixToImage(data):\n",
    "    data = data*255\n",
    "    new_im = Image.fromarray(data.astype(np.uint8))\n",
    "    return new_im\n",
    "\n",
    "\n",
    "new_im = MatrixToImage(data)\n",
    "\n",
    "new_im.show()\n",
    "#會跳出一張預測的照片，圖片顯示的根原本的圖形類似，確實是有預測成0(原本是一個長方形的框框)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TA:\n",
    "You are supposed to predict the digit you drew by using the model trained in 1(a). Not retrain a new model.  \n",
    "You may run the following code instead.\n",
    "\n",
    "```python\n",
    "import os \n",
    "from PIL import Image\n",
    "path = \"DecisionTreeClassifier-with-scikit-learn-2021S-data\"\n",
    "                                                 ~~~~ ---> 2022\n",
    "filename = os.path.join(path, \"my_digit.png\")\n",
    "img = Image.open(filename).resize((8,8)).convert(\"L\")\n",
    "\n",
    "arr = np.array(img).ravel()\n",
    "arr = (255 - arr) * 16 / 255\n",
    "X_sample = arr[np.newaxis, :]\n",
    "model.predict(X_sample)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 4\n",
    "Let  \n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "```  \n",
    "and `model` be your decision tree classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4(a)\n",
    "Plot the decision tree with the keyword `node_ids=True` .  \n",
    "(If necessary, you may use `plt.figure(figsize=(15,15))` to change the figure size.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "import sklearn.model_selection\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# 80% training and 20% test\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_new = model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plot_tree(model,node_ids=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4(b)\n",
    "Let `T = model.tree_` .  \n",
    "Print `T.children_left` and `T.children_right` .  \n",
    "Compare these two arrays with the decision tree you printed.  \n",
    "What do they mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "T = model.tree_\n",
    "print(T.children_left)\n",
    "print(T.children_right)\n",
    "# Look at the array,in each row means the node of the left child and right child\n",
    "# the i-th row means the left child node and right child node of the (i-1)-th node\n",
    "# -1 means in (i-1)-th node it has no children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4(c)\n",
    "Print `T.feature` and `T.threshold` .  \n",
    "Compare these two arrays with the decision tree you printed.  \n",
    "What do they mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "print(T.feature)\n",
    "# T.feature[i] means when we are classifing the i-th node,\n",
    "# it use the charateristic of feature[i]-th to start classification\n",
    "\n",
    "print(T.threshold)\n",
    "# When we are classifing, threshold is the critical value in every node "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4(d)\n",
    "Print `T.n_node_samples` and `T.impurity` .  \n",
    "Compare these two arrays with the decision tree you printed.  \n",
    "What do they mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "print(T.n_node_samples)\n",
    "# T.n_node_samples[i] means that\n",
    "# in i-th node, the numbers of the sample will equal to \"T.n_node_samples[i]\"\n",
    "print(T.impurity)\n",
    "# When the values of the \"T.impurity\" is not equal to 0, it means the data inside are still not pure \n",
    "#so it requires to find the feature to classify more precisely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4(e)\n",
    "For each `i = 0,1,2,3`, count how many nodes uses feature `i` for splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "for i in range(4):\n",
    "    print(\"Feature \",i,\" uses \",sum(T.feature==i),\" times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4(f)\n",
    "Suppose there are $N$ sample points in the training data.  \n",
    "Suppose a node contains $n_s$ sample points.  \n",
    "Within these sample points, there is a chance of $p_i$ to get class $i$.  \n",
    "One may calculate the impurity $H$ (Gini or entropy) at this point.  \n",
    "\n",
    "Suppose the \"information\" at each node is  \n",
    "$$I = \\frac{n_s}{N}\\cdot H.$$\n",
    "Calculate the information at each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "ns = T.n_node_samples\n",
    "H = T.impurity\n",
    "N = 120\n",
    "I = (ns/N)*H\n",
    "for i in range(13):\n",
    "    print(\"The information of node \",i,\"is \",I[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4(g)\n",
    "Suppose $I$ is the information at one node, while $I_\\ell$ and $I_r$ are the information at its left and right children, respectively.  \n",
    "The **information gain** at this node is $I_\\ell + I_r -I$.  \n",
    "Calculate the information gain at each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "inf_g = np.zeros_like(I)\n",
    "for i in range(13):\n",
    "    left = T.children_left[i]\n",
    "    right = T.children_right[i]\n",
    "    if left == -1:\n",
    "        if right == -1:\n",
    "            inf_g[i] = -I[i]\n",
    "            inf_g[i] = I[right] - I[i]\n",
    "            inf_g[i] = I[left] + I[right] - I[i]\n",
    "    print(\"The information gain of node \",i,\" is: \",inf_g[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4(h)\n",
    "Let $W_i$ be the sum of information gain among nodes using feature $i$ for splitting.  \n",
    "Calculate an array `W` such whose entries are `W_i` for each feature $i$.  \n",
    "Let `W = W / W.sum()` .  \n",
    "Compare `W` with `model.feature_importances_` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "feature = T.feature\n",
    "W = np.zeros(4)\n",
    "\n",
    "where0 = np.where(feature==0)\n",
    "where1 = np.where(feature==1)\n",
    "where2 = np.where(feature==2)\n",
    "where3 = np.where(feature==3)\n",
    "\n",
    "for i in range(4):\n",
    "    inf_where = np.where(feature==i)\n",
    "    W[i] = sum(inf_g[inf_where])\n",
    "\n",
    "W = W / W.sum()\n",
    "print(model.feature_importances_)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 5\n",
    "Let  \n",
    "```python \n",
    "X = 5 * np.random.randn(1000,2)\n",
    "lengths = np.linalg.norm(X, axis=1)\n",
    "band1 = (lengths > 1) & (lengths <2)  \n",
    "band2 = (lengths > 3) & (lengths <4)\n",
    "X = np.vstack([X[band1], X[band2]])\n",
    "y = np.array([0]*band1.sum() + [1]*band2.sum())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5(a)\n",
    "Go through the split-train-test process.  \n",
    "What is the accuracy score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = 5 * np.random.randn(1000,2)\n",
    "lengths = np.linalg.norm(X, axis=1)\n",
    "band1 = (lengths > 1) & (lengths <2)  \n",
    "band2 = (lengths > 3) & (lengths <4)\n",
    "X = np.vstack([X[band1], X[band2]])\n",
    "y = np.array([0]*band1.sum() + [1]*band2.sum())\n",
    "\n",
    "score = np.array([])\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X, y)\n",
    "    y_new = model.predict(X_test)\n",
    "    \n",
    "    scr = np.sum(y_new==y_test)/y_test.shape[0]\n",
    "    score = np.append(score,scr)\n",
    "for i in range(10):\n",
    "    print(\"test\",i+1,\": score =\",score[i])\n",
    "print(\"average score =\",np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5(b)\n",
    "Use some random points to plot the regions for each class.  \n",
    "(Just as what we did in Exercise 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "mu = np.array([1,1])\n",
    "cov = np.array([[1.1,-1],\n",
    "                [-1,1.1]])\n",
    "X = np.vstack([np.random.multivariate_normal(mu, cov, 100), \n",
    "               np.random.multivariate_normal(-mu, cov, 100)])\n",
    "y = np.array([0]*100 + [1]*100)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "X_test = np.random.rand(1000,2) * 10 - np.array([5,5])  ##創建測試集的資料\n",
    "y_new = model.predict(X_test)\n",
    "\n",
    "\n",
    "#plt.scatter(*X_test.T, c=y_new, s=10, alpha=0.3) \n",
    "X_test=np.random.rand(1000,2)*10 - np.array([5,5])\n",
    "y_new = model.predict(X_test)\n",
    "plt.axis('equal')\n",
    "plt.scatter(X_test.T[0],X_test.T[1],c=y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#end#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 上課畫的impurity line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = \"gini\"\n",
    "\n",
    "def impurity(arr):\n",
    "    dtrib = np.unique(arr, return_counts=True)[1]\n",
    "    dtrib = dtrib / dtrib.sum()\n",
    "    if criteria == \"gini\":\n",
    "        return np.sum(dtrib * (1 - dtrib))\n",
    "    if criteria == \"entropy\":\n",
    "        return np.sum(-dtrib * np.log2(dtrib))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((10,2), dtype=float)\n",
    "X[:,0] = np.arange(10)\n",
    "y = np.array([0,0,0,1,1,0,1,1,1,1])\n",
    "\n",
    "plt.scatter(*X.T, c=y)\n",
    "\n",
    "Iprime = np.zeros_like(X)\n",
    "### for illustration, just focus on x-coordinate\n",
    "j = 0 \n",
    "N,d = X.shape\n",
    "\n",
    "for i in range(N):\n",
    "    mask = (X[:,j] <= X[i,j])\n",
    "    NL, NR = mask.sum(), (~mask).sum()\n",
    "    HL = impurity(y[mask])\n",
    "    HR = impurity(y[~mask])\n",
    "    Iprime[i,j] = (NL * HL + NR * HR) / N\n",
    "    \n",
    "plt.plot(X[:,j], Iprime[:,j])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
