{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PolynomialRegression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Creative Commons License](https://i.creativecommons.org/l/by/4.0/88x31.png)  \n",
    "This work by Jephian Lin is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "**Input:**  \n",
    "- `X`: an array of shape `(N,1)` whose rows are samples and columns are features\n",
    "- `y`: the labels of shape `(N,)`\n",
    "- `degree`: the degree of the polynomial \n",
    "- `**kwargs`: keywords for your linear regression function\n",
    "\n",
    "**Output:**  \n",
    "Revised output of your linear regression function.\n",
    "\n",
    "**Steps:**\n",
    "1. Let `X_ex = X**np.arange(1, degree + 1)` .\n",
    "2. Suppose `LR` is your linear regression fuction.  \n",
    "Let `predict_lin,coef,intercept = LR(X_ex, y, **kwargs)` .  \n",
    "3. Define the function `predict` that sends `X_test` to `(X_test**np.arange(1, degree+1)).dot(coef) + intercept` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode\n",
    "Translate the algorithm into the pseudocode.  \n",
    "This helps you to identify the parts that you don't know how to do it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. \n",
    "    2. \n",
    "    3. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "class PolynomialRegression:\n",
    "    def __init__(self, degree):\n",
    "        self.degree = degree\n",
    "    \n",
    "    def LR_proj(self, x, y, regularization=None, alpha=1e-4):\n",
    "        A = x\n",
    "        v = np.linalg.inv(A.T.dot(A)).dot(A.T.dot(y))\n",
    "        return v[0], v[1:]\n",
    "    \n",
    "    def LR_gd(self, x, y, n_iter=100, learning_rate=1e-4, regularization=None, alpha=1e-4, verbose=False):\n",
    "        self.scores = []\n",
    "        A = x\n",
    "        v = np.random.rand(A.shape[1],)\n",
    "        for i in range(n_iter):\n",
    "            d = 2/X.shape[0]*(A.dot(v)-y).T.dot(A)\n",
    "            if regularization == 'L1':\n",
    "                d = d + alpha * np.sign(v)\n",
    "            if regularization == 'L2':\n",
    "                d = d + alpha * 2 * v\n",
    "            v = v - learning_rate*d\n",
    "            y_pred = A[:, 1:].dot(v[1:]) + v[0]\n",
    "            self.scores.append(mean_absolute_error(y, y_pred))\n",
    "            if verbose:\n",
    "                print(self.scores[-1])\n",
    "        return v[0], v[1:]\n",
    "    \n",
    "    def fit(self, x, y, algorithm='projection', regularization=None, verbose=False):\n",
    "        self.powers = [x for x in range(self.degree+1)]\n",
    "        self.X = x**self.powers\n",
    "        self.y = y\n",
    "        if algorithm == 'projection':\n",
    "            self.intercept_, self.coef_ = self.LR_proj(self.X, y, regularization)\n",
    "        if algorithm == 'grad_descent':\n",
    "            self.intercept_, self.coef_ = self.LR_gd(self.X, y, 100, 1e-4, regularization, 1e-4, verbose)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        X = x**self.powers[1:]\n",
    "        return X.dot(self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alex: \n",
    "Your function cannot adjust `n_iter`, `learning_rate`, `alpha` since you fix these values in `fit`. Also, you should include `fit_intercept` as a parameter.\n",
    "\n",
    "You should not use the package. You should calculate the error by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "Take some sample data from [PolynomialRegression-with-scikit-learn](PolynomialRegression-with-scikit-learn.ipynb) and check if your code generates similar outputs with the existing packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Name of the data\n",
    "Description of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### results with your code\n",
    "x = np.arange(10)\n",
    "X = x[:,np.newaxis]\n",
    "y = 0.1*x**2 + 0.2*x + 0.3 + 0.5*np.random.randn(10)\n",
    "\n",
    "x_test = np.linspace(0,10,20)\n",
    "X_test = x_test[:,np.newaxis]\n",
    "\n",
    "model = PolynomialRegression(2)\n",
    "model.fit(X, y)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(model.coef_)\n",
    "print(model.intercept_)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### results with existing packages\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def SKPolynomialRegression(degree=2, fit_intercept=True):\n",
    "    return make_pipeline(PolynomialFeatures(degree=degree, include_bias=False), \n",
    "                         LinearRegression(fit_intercept=fit_intercept))\n",
    "\n",
    "model = SKPolynomialRegression(2)\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(model[1].coef_)\n",
    "print(model[1].intercept_)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 1\n",
    "Let  \n",
    "```python\n",
    "degree = 3\n",
    "x = np.arange(5)\n",
    "X = x[:,np.newaxis]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1(a)\n",
    "Let `X_ex1 = X**np.arange(1, degree+1)` .  \n",
    "The new data `X_ex1` is supposed to be the same as the output of `sklearn.preprocessing.PolynomialFeatures` with `include_bias=False` .  \n",
    "Check if this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "degree = 3\n",
    "x = np.arange(5)\n",
    "X = x[:,np.newaxis]\n",
    "X_ex1 = X**np.arange(1, degree+1)\n",
    "\n",
    "ftr_model = PolynomialFeatures(degree, include_bias=False)\n",
    "sk_X_ex1 = ftr_model.fit_transform(X)\n",
    "print(X_ex1)\n",
    "print(sk_X_ex1)\n",
    "print('It is ture about X_ex1 be the same as the output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1(b)\n",
    "Let `X_ex1 = X**np.arange(0, degree+1)` .  \n",
    "The new data `X_ex1` is supposed to be the same as the output of `sklearn.preprocessing.PolynomialFeatures` with `include_bias=False` .  \n",
    "Check if this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "degree = 3\n",
    "x = np.arange(5)\n",
    "X = x[:,np.newaxis]\n",
    "X_ex1 = X**np.arange(0, degree+1)\n",
    "\n",
    "ftr_model = PolynomialFeatures(degree, include_bias=False)\n",
    "sk_X_ex1 = ftr_model.fit_transform(X)\n",
    "print(X_ex1)\n",
    "print(sk_X_ex1)\n",
    "print('X_ex1 is not same as sk_X_ex1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alex:\n",
    "It would be the same if you set `include_bias=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 2\n",
    "Let  \n",
    "```python\n",
    "x = np.arange(10)\n",
    "y = 0.1*x**2 + 0.2*x + 0.3 + 0.5*np.random.randn(10)\n",
    "X = x[:,np.newaxis]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2(a)\n",
    "Let `degree=2` .\n",
    "Apply the linear regresssion algorithm to `X`  \n",
    "1. by your code with `algorithm==\"projection\"` ,  \n",
    "2. by your code with `algorithm==\"grad_descent\"` ,  \n",
    "3. by `sklearn.linear_model.LinearRegresssion` .  \n",
    "\n",
    "Check if the outputs are almost the same (up to some numerical errors).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "x = np.arange(10)\n",
    "y = 0.1*x**2 + 0.2*x + 0.3 + 0.5*np.random.randn(10)\n",
    "X = x[:,np.newaxis]\n",
    "degree = 2\n",
    "\n",
    "model = PolynomialRegression(2)\n",
    "model.fit(X, y, algorithm='projection')\n",
    "proj_y_pred = model.predict(X)\n",
    "\n",
    "model.fit(X, y, algorithm='grad_descent', verbose=False)\n",
    "gd_y_pred = model.predict(X)\n",
    "\n",
    "sk_model = SKPolynomialRegression(2)\n",
    "sk_model.fit(X, y)\n",
    "sk_y_pred = sk_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(proj_y_pred)\n",
    "print(gd_y_pred)\n",
    "print(sk_y_pred)\n",
    "print('1跟3是一樣的')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alex:\n",
    "You can adjust the `n_iter` and `learning_rate` to make the second result to be the same as the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2(b)\n",
    "Modify your code so that it prints the mean square error at each step of the gradient descent.  \n",
    "Check if it is always decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "plt.plot(list(range(100)), model.scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alex:\n",
    "Your `model.scores` stores mean absolute error, but you are supposed to be print out the mean square error. Also, it is hard to see whether the errors are always decreasing by the figure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3\n",
    "Add a new keyword `regularization`, which can be `None`, `\"L1\"`, or `\"L2\"` .  \n",
    "Add another keyword `alpha`, which is a positive number.  \n",
    "\n",
    "When `regularization==None`, the cost function is \n",
    "$$\\frac{1}{N}\\sum_{i=0}^{N-1}\\|f({\\bf x}_i) - y_i\\|^2.$$ \n",
    "When `regularization==\"L1\"`, the cost function is \n",
    "$$\\frac{1}{N}\\sum_{i=0}^{N-1}\\|f({\\bf x}_i) - y_i\\|^2 + \\alpha\\sum_{i=0}^{d-1}|c_i|.$$ \n",
    "When `regularization==\"L2\"`, the cost function is \n",
    "$$\\frac{1}{N}\\sum_{i=0}^{N-1}\\|f({\\bf x}_i) - y_i\\|^2 + \\alpha\\sum_{i=0}^{d-1}c_i^2.$$ \n",
    "Here ${\\bf x}_i$ are the data, $y_i$ are the labels, and $c_i$ are the coefficients to be solved.\n",
    "\n",
    "The regularization avoids the coefficients being too high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3(a)\n",
    "When `regularization==\"L1\"`, the correct gradient is `g = g0 + alpha * np.sign(c)` , where `g0` is the gradient when `regularization==None` .  \n",
    "Update your code for L1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "x = np.arange(10)\n",
    "y = 0.1*x**2 + 0.2*x + 0.3 + 0.5*np.random.randn(10)\n",
    "X = x[:,np.newaxis]\n",
    "degree = 2\n",
    "\n",
    "model = PolynomialRegression(2)\n",
    "model.fit(X, y, algorithm='grad_descent', regularization='L1', verbose=False)\n",
    "l1_gd_y_pred = model.predict(X)\n",
    "\n",
    "sk_model = SKPolynomialRegression(2)\n",
    "sk_model.fit(X, y)\n",
    "sk_y_pred = sk_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l1_gd_y_pred)\n",
    "print(sk_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3(b)\n",
    "When `regularization==\"L2\"`, the correct gradient is `g = g0 + alpha * 2 * c` , where `g0` is the gradient when `regularization==None` .  \n",
    "Update your code for L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "model.fit(X, y, algorithm='grad_descent', regularization='L2', verbose=False)\n",
    "l2_gd_y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l2_gd_y_pred)\n",
    "print(sk_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
