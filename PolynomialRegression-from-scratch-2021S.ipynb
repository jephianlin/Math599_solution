{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PolynomialRegression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Creative Commons License](https://i.creativecommons.org/l/by/4.0/88x31.png)  \n",
    "This work by Jephian Lin is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "**Input:**  \n",
    "- `X`: an array of shape `(N,1)` whose rows are samples and columns are features\n",
    "- `y`: the labels of shape `(N,)`\n",
    "- `degree`: the degree of the polynomial \n",
    "- `**kwargs`: keywords for your linear regression function\n",
    "\n",
    "**Output:**  \n",
    "Revised output of your linear regression function.\n",
    "\n",
    "**Steps:**\n",
    "1. Let `X_ex = X**np.arange(1, degree + 1)` .\n",
    "2. Suppose `LR` is your linear regression fuction.  \n",
    "Let `predict_lin,coef,intercetp = LR(X_ex, **kwargs)` .  \n",
    "3. Define the function `predict` that sends `X_test` to `(X_test**np.arange(1, degree+1)).dot(coef) + intercept` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode\n",
    "Translate the algorithm into the pseudocode.  \n",
    "This helps you to identify the parts that you don't know how to do it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. \n",
    "    2. \n",
    "    3. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class MyPolynomialRegression():\n",
    "    def __init__(self, degree, fit_intercept=True, algorithm=\"projection\", learning_rate=0.01, n_iter=10000, regularization=None, alpha=1):\n",
    "        self.degree = degree\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.algorithm = algorithm\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "        self.regularization = regularization\n",
    "        self.alpha = alpha\n",
    "        self.coef_ = 0\n",
    "        self.intercept_ = 0\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return (X**np.arange(1, self.degree+1)).dot(self.coef_) + self.intercept_\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X_ex = X**np.arange(1, self.degree + 1)\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            A = np.hstack([np.ones([X_ex.shape[0],1]), X_ex])\n",
    "        else:\n",
    "            A = X_ex.copy()\n",
    "            \n",
    "        N = A.shape[0]\n",
    "        dp = A.shape[1]\n",
    "        \n",
    "        if self.algorithm == \"projection\":\n",
    "            v = np.linalg.inv(A.T.dot(A)).dot(A.T).dot(y)\n",
    "        if self.algorithm == \"grad_descent\":\n",
    "            v = np.random.randn(dp)\n",
    "            \n",
    "            self.fit_score = np.zeros(self.n_iter)\n",
    "            self.fit_MSE = np.zeros(self.n_iter)\n",
    "        \n",
    "            if self.fit_intercept:\n",
    "                for i in range(self.n_iter):\n",
    "                    self.coef_ = v[1:]\n",
    "                    self.intercept_ = v[0]\n",
    "                    self.fit_score[i] = self.score(X, y)\n",
    "                    self.fit_MSE[i] = mean_squared_error(y, self.predict(X))\n",
    "                    gradient = (2/N)*((A.dot(v)-y).T.dot(A))\n",
    "                    if self.regularization == \"L1\":\n",
    "                        gradient = gradient + self.alpha * np.sign(v)\n",
    "                    if self.regularization == \"L2\":\n",
    "                        gradient = gradient + self.alpha * 2 * v\n",
    "                    v = v - self.learning_rate * gradient\n",
    "            else:\n",
    "                for i in range(self.n_iter):\n",
    "                    self.coef_ = v\n",
    "                    self.intercept_ = 0.0\n",
    "                    self.fit_score[i] = self.score(X, y)\n",
    "                    self.fit_MSE[i] = mean_squared_error(y, self.predict(X))\n",
    "                    gradient = (2/N)*((A.dot(v)-y).T.dot(A))\n",
    "                    if self.regularization == \"L1\":\n",
    "                        gradient = gradient + self.alpha * np.sign(v)\n",
    "                    if self.regularization == \"L2\":\n",
    "                        gradient = gradient + self.alpha * 2 * v\n",
    "                    v = v - self.learning_rate * gradient\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            self.coef_ = v[1:]\n",
    "            self.intercept_ = v[0]\n",
    "        else:\n",
    "            self.coef_ = v\n",
    "            self.intercept_ = 0.0\n",
    "        \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_new = self.predict(X)\n",
    "        R2 = 1-((y-y_new)**2).sum()/((y-y.mean())**2).sum()\n",
    "        return R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "Take some sample data from [PolynomialRegression-with-scikit-learn](PolynomialRegression-with-scikit-learn.ipynb) and check if your code generates similar outputs with the existing packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Name of the data\n",
    "Description of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### results with your code\n",
    "\n",
    "x = np.arange(10)\n",
    "y = 0.1*x**2 + 0.2*x + 0.3 + 0.5*np.random.randn(10)\n",
    "X = x[:,np.newaxis]\n",
    "x_test = np.linspace(0,10,20)\n",
    "X_test = x_test[:,np.newaxis]\n",
    "\n",
    "model = MyPolynomialRegression(2)\n",
    "model.fit(X, y)\n",
    "y_new = model.predict(X_test)\n",
    "plt.scatter(x,y)\n",
    "plt.plot( x_test , y_new, c='r')\n",
    "\n",
    "print(model.coef_)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### results with existing packages\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def PolynomialRegression(degree=2, fit_intercept=True):\n",
    "    return make_pipeline(PolynomialFeatures(degree=degree, include_bias=False), \n",
    "                         LinearRegression(fit_intercept=fit_intercept))\n",
    "\n",
    "model = PolynomialRegression(2)\n",
    "model.fit(X, y)\n",
    "y_new = model.predict(X_test)\n",
    "plt.scatter(x,y)\n",
    "plt.plot( x_test , y_new, c='r') \n",
    "\n",
    "print(model[1].coef_)\n",
    "print(model[1].intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 1\n",
    "Let  \n",
    "```python\n",
    "degree = 3\n",
    "x = np.arange(5)\n",
    "X = x[:,np.newaxis]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1(a)\n",
    "Let `X_ex1 = X**np.arange(1, degree+1)` .  \n",
    "The new data `X_ex1` is supposed to be the same as the output of `sklearn.preprocessing.PolynomialFeatures` with `include_bias=False` .  \n",
    "Check if this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x = np.arange(10)\n",
    "X = x[:,np.newaxis]\n",
    "degree = 2\n",
    "\n",
    "model = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "X_ex = model.fit_transform(X)\n",
    "\n",
    "X_ex1 = X**np.arange(1, degree+1)\n",
    "print(X_ex1 == X_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1(b)\n",
    "Let `X_ex1 = X**np.arange(0, degree+1)` .  \n",
    "The new data `X_ex1` is supposed to be the same as the output of `sklearn.preprocessing.PolynomialFeatures` with `include_bias=True` .  \n",
    "Check if this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "model = PolynomialFeatures(degree=degree, include_bias=True)\n",
    "X_ex = model.fit_transform(X)\n",
    "\n",
    "X_ex1 = X**np.arange(0, degree+1)\n",
    "print(X_ex1 == X_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 2\n",
    "Let  \n",
    "```python\n",
    "x = np.arange(10)\n",
    "y = 0.1*x**2 + 0.2*x + 0.3 + 0.5*np.random.randn(10)\n",
    "X = x[:,np.newaxis]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2(a)\n",
    "Let `degree=2` .\n",
    "Apply the linear regresssion algorithm to `X`  \n",
    "1. by your code with `algorithm==\"projection\"` ,  \n",
    "2. by your code with `algorithm==\"grad_descent\"` ,  \n",
    "3. by `sklearn.linear_model.LinearRegresssion` .  \n",
    "\n",
    "Check if the outputs are almost the same (up to some numerical errors).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "model = MyPolynomialRegression(degree=2, algorithm=\"projection\")\n",
    "model.fit(X, y)\n",
    "print(\"projection:\")\n",
    "print(model.coef_)\n",
    "print(model.intercept_)\n",
    "\n",
    "model = MyPolynomialRegression(degree=2, algorithm=\"grad_descent\", learning_rate=0.0006, n_iter=30000)\n",
    "model.fit(X, y)\n",
    "print(\"grad_descent:\")\n",
    "print(model.coef_)\n",
    "print(model.intercept_)\n",
    "\n",
    "model = PolynomialRegression(2)\n",
    "model.fit(X, y)\n",
    "print(\"sklearn:\")\n",
    "print(model[1].coef_)\n",
    "print(model[1].intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2(b)\n",
    "Modify your code so that it prints the mean square error at each step of the gradient descent.  \n",
    "Check if it is always decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "x = np.arange(10)\n",
    "y = 0.1*x**2 + 0.2*x + 0.3 + 0.5*np.random.randn(10)\n",
    "X = x[:,np.newaxis]\n",
    "\n",
    "model = MyPolynomialRegression(degree=2, algorithm=\"grad_descent\", learning_rate=0.0006, n_iter=30000)\n",
    "model.fit(X, y)\n",
    "print(model.fit_MSE[:10:1])\n",
    "plt.scatter(np.arange(model.n_iter), model.fit_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3\n",
    "Add a new keyword `regularization`, which can be `None`, `\"L1\"`, or `\"L2\"` .  \n",
    "Add another keyword `alpha`, which is a positive number.  \n",
    "\n",
    "When `regularization==None`, the cost function is \n",
    "$$\\frac{1}{N}\\sum_{i=0}^{N-1}\\|f({\\bf x}_i) - y_i\\|^2.$$ \n",
    "When `regularization==\"L1\"`, the cost function is \n",
    "$$\\frac{1}{N}\\sum_{i=0}^{N-1}\\|f({\\bf x}_i) - y_i\\|^2 + \\sum_{i=0}^{d-1}|c_i|.$$ \n",
    "When `regularization==\"L2\"`, the cost function is \n",
    "$$\\frac{1}{N}\\sum_{i=0}^{N-1}\\|f({\\bf x}_i) - y_i\\|^2 + \\sum_{i=0}^{d-1}c_i^2.$$ \n",
    "Here ${\\bf x}_i$ are the data, $y_i$ are the labels, and $c_i$ are the coefficients to be solved.\n",
    "\n",
    "The regularization avoids the coefficients being too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyPolynomialRegression(degree=2, algorithm=\"grad_descent\", learning_rate=0.0006, n_iter=30000)\n",
    "model.fit(X, y)\n",
    "print(model.coef_)\n",
    "print(model.intercept_)\n",
    "print(\"MSE:\", model.fit_MSE[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3(a)\n",
    "When `regularization==\"L1\"`, the correct gradient is `g = g0 + alpha * np.sign(c)` , where `g0` is the gradient when `regularization==None` .  \n",
    "Update your code for L1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "model = MyPolynomialRegression(degree=2, algorithm=\"grad_descent\", learning_rate=0.0006, n_iter=30000, regularization=\"L1\")\n",
    "model.fit(X, y)\n",
    "print(model.coef_)\n",
    "print(model.intercept_)\n",
    "print(\"MSE:\", model.fit_MSE[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3(b)\n",
    "When `regularization==\"L2\"`, the correct gradient is `g = g0 + alpha * 2 * v` , where `g0` is the gradient when `regularization==None` .  \n",
    "Update your code for L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "model = MyPolynomialRegression(degree=2, algorithm=\"grad_descent\", learning_rate=0.0006, n_iter=30000, regularization=\"L2\")\n",
    "model.fit(X, y)\n",
    "print(model.coef_)\n",
    "print(model.intercept_)\n",
    "print(\"MSE:\", model.fit_MSE[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Jephian:\n",
    "Wonderful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
