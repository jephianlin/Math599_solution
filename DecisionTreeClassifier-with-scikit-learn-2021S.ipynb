{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier with scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier ### added by Jephian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Creative Commons License](https://i.creativecommons.org/l/by/4.0/88x31.png)  \n",
    "This work by Jephian Lin is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(<parameters)\n",
    "model.fit(X, y)\n",
    "y_new = model.predict(X_test)\n",
    "```\n",
    "\n",
    "[Official Reference](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "- `criterion`: `\"gini\"` or `\"entropy\"`  \n",
    "the function to measure a good cut\n",
    "- `max_depth`: an integer, the maximum depth of a tree\n",
    "- `min_samples_split`: if a node has more than `min_samples_split` samples, then split further\n",
    "\n",
    "and many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes\n",
    "- `classes_`: an array of shape `(n_classes,)`  \n",
    "(Usually `0, ..., n_classes-1`)\n",
    "- `feature_importances_`: an array of shape `(n_features,)`  \n",
    "the total importance (impurity reduction) of each feature\n",
    "- `tree_`: the constructed decition tree\n",
    "\n",
    "For `model.tree_`  \n",
    "- `children_left[i]`: id of the left child of node i or -1 if leaf node\n",
    "- `children_right[i]`: id of the right child of node i or -1 if leaf node\n",
    "- `feature[i]`: feature used for splitting node i\n",
    "- `threshold[i]`: threshold value at node i\n",
    "- `n_node_samples[i]`: the number of of training samples reaching node i\n",
    "- `impurity[i]`: the impurity at node i\n",
    "\n",
    "(Source: [Understanding the decision tree structure](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 1\n",
    "Let  \n",
    "```python\n",
    "mu = np.array([1,1])\n",
    "cov = np.array([[1.1,-1],\n",
    "                [-1,1.1]])\n",
    "X = np.vstack([np.random.multivariate_normal(mu, cov, 100), \n",
    "               np.random.multivariate_normal(-mu, cov, 100)])\n",
    "y = np.array([0]*100 + [1]*100)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1(a)\n",
    "Plot the points (rows) in `X` with `c=y` .  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "mu = np.array([1,1])\n",
    "cov = np.array([[1.1,-1],\n",
    "                [-1,1.1]])\n",
    "X = np.vstack([np.random.multivariate_normal(mu, cov, 100), \n",
    "               np.random.multivariate_normal(-mu, cov, 100)])\n",
    "y = np.array([0]*100 + [1]*100)\n",
    "\n",
    "# plot\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1(b)\n",
    "Draw 1000 random points uniformly on the region $-5\\leq x\\leq 5$ and $-5\\leq y\\leq 5$.  \n",
    "Use the trained model to make a prediction `y_new` .  \n",
    "Then plot these 1000 points with `c=y_new` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "X_test = (np.random.rand(1000, 2)*10)-5\n",
    "\n",
    "# model predict\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "y_new = model.predict(X_test)\n",
    "\n",
    "# plot\n",
    "plt.scatter(X_test[:,0], X_test[:,1], c=y_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1(c)\n",
    "Trains a decision tree model.  \n",
    "Let  \n",
    "```python\n",
    "from sklearn.tree import plot_tree\n",
    "plot_tree(model)\n",
    "```\n",
    "Try to understand the following questions:\n",
    "- Check if the number of samples in a node is equal to the sum of those in its two children\n",
    "- What can you say about the `gini` value at each leaf node?\n",
    "- What can you say about the `value` distribution at each leaf node?\n",
    "- Check how many samples satisfies the criteria at the root node.  It should be the same as the number of samples in the left child (of the root)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### your answer here\n",
    "from sklearn.tree import plot_tree\n",
    "plot_tree(model, class_names=['0','1'])\n",
    "\n",
    "# ANS:\n",
    "# 1. Yes, the number of samples in a node is equal to the sum of those in its two children(ex: 108+92=200, 90+18=108, etc.)\n",
    "\n",
    "# 2. \"gini\" means the degree of impurity of the dataset. For example, there are 200 data points in the dataset. After the first\n",
    "#    classification, the model say that there are 100 points labeled as 0, 100 points labeled as 1. 100 points are correct, and\n",
    "#    100 are mislabeled, so the gini=0.5 (formula: https://www.datacamp.com/community/tutorials/decision-tree-classification-python)\n",
    "#    Only if the gini=0 will the model stop classifying. gini=0 means that the node is pure.\n",
    "\n",
    "# 3. \"value\" means the number of data points in each class (以這題來說就是多少點是被標0，多少被標1). Take the left child at \n",
    "#    the second level as example, value=[18,90] means that 18 points should be labeled as 0, but they are mislabeles as 1.\n",
    "#    Only if value=[0, xx] or [xx, 0] will the model stop classifying. It means that there are no points mislabeled.\n",
    "\n",
    "# 4. There are 108 points satisfies the criteria at the root node, 92 don't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Jephian:\n",
    "Nice answers.  \n",
    "For 2, my intention was to have a observation that each node have the Gini impurity equal to zero.  \n",
    "For 4, you may use the code below and see if the output is the same as the number of samples in the left child of the root.  \n",
    "```python\n",
    "j = model.tree_.feature[0]\n",
    "t = model.tree_.threshold[0]\n",
    "### x[j] <= t\n",
    "(X[:,j] < t).sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 2\n",
    "Let  \n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2(a)\n",
    "Apply the decision tree classification algorithm to `X` and `y` .  \n",
    "Make a prediciont of the training data.  \n",
    "How is the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "from sklearn.datasets import load_iris\n",
    "import sklearn.model_selection\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_new = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_new))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2(b)\n",
    "Plot the decision tree.  \n",
    "If a bag of $n$ balls contains $n_i$ balls of color $i$ (for colors $i=0,\\ldots,c-1$), the **Gini impurity** of this bag (distribution) is  \n",
    "$$\\sum_{i=0}^{c-1} p_i(1 - p_i),$$\n",
    "where $p_i = n_i/n$ is the probability of getting a ball of color $i$.\n",
    "\n",
    "Pick a node and check if this formula is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "from sklearn.tree import plot_tree\n",
    "plot_tree(model)\n",
    "(36/105)*(1-36/105) + (32/105)*(1-32/105) + (37/105)*(1-37/105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2(c)\n",
    "Change the model setting to `criteria=\"entropy\"` and plot the decision tree again.  \n",
    "If a bag of $n$ balls contains $n_i$ balls of color $i$ (for colors $i=0,\\ldots,c-1$), the **entropy** of this bag (distribution) is  \n",
    "$$\\sum_{i=0}^{c-1} -p_i\\log_2(p_i),$$\n",
    "where $p_i = n_i/n$ is the probability of getting a ball of color $i$.\n",
    "\n",
    "Pick a node and check if this formula is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "model.fit(X_train, y_train)\n",
    "y_new = model.predict(X_test)\n",
    "\n",
    "plot_tree(model)\n",
    "\n",
    "-(36/105)*math.log((36/105),2)+-1*(34/69)*math.log((34/69),2)+-1*(35/69)*math.log((35/69),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "-(36/105)*log(36/105,2) - (32/105)*log(32/105,2) - (37/105)*log(37/105,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Jephian:\n",
    "You may use `np.log2` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3\n",
    "Let  \n",
    "```python\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "mask = (digits.target == 0) | (digits.target == 1)\n",
    "X = digits.data[mask]\n",
    "y = digits.target[mask]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3(a)\n",
    "Train a decision tree classification model.  \n",
    "How is its accuracy score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "digits = load_digits()\n",
    "mask = (digits.target == 0) | (digits.target == 1)\n",
    "X = digits.data[mask]\n",
    "y = digits.target[mask]\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "y_new = model.predict(X)\n",
    "print(\"Testing data accuracy:\",metrics.accuracy_score(y, y_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3(b)\n",
    "Use any software or online app to draw a picture of 0 or 1.  \n",
    "Save it as a file, e.g., `my_digit.png` .  \n",
    "Use the following code to load it.  \n",
    "```python\n",
    "from PIL import Image\n",
    "img = Image.open(\"my_digit.png\").resize(8,8)\n",
    "```\n",
    "Does the model give you the right answer?  \n",
    "Each of you can do 5 pictures.  \n",
    "Let's see what is the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os ### added by Jephian\n",
    "path = \"DecisionTreeClassifier-with-scikit-learn-2021S-data\" ### added by Jephian\n",
    "\n",
    "### your answer here\n",
    "from PIL import Image\n",
    "def ImageToMatrix(filename):\n",
    "    # 读取图片\n",
    "    im = Image.open(filename).resize((8,8))\n",
    " \n",
    "    width,height = im.size\n",
    "    im = im.convert(\"L\") \n",
    "    data = im.getdata()\n",
    "    data = np.array(data,dtype='float')/255.0\n",
    "    #new_data = np.reshape(data,(width,height))\n",
    "    new_data = np.reshape(data,(height,width))\n",
    "    return new_data\n",
    "\n",
    "filename = os.path.join(path, \"my_digit.png\") ### added by Jephian\n",
    "data = ImageToMatrix(filename) ### added by Jephian\n",
    "# data = ImageToMatrix(\"my_digit.png\")\n",
    "\n",
    "data[data < 1] = 0\n",
    "\n",
    "mask0 = (data[:,6] == 0)\n",
    "mask1 = (data[:,6] == 1)\n",
    "X = data[:,mask0]\n",
    "y = data[:,mask1]\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "y_new = model.predict(X)\n",
    "print(\"Testing data accuracy:\",metrics.accuracy_score(y, y_new))\n",
    "\n",
    "\n",
    "def MatrixToImage(data):\n",
    "    data = data*255\n",
    "    new_im = Image.fromarray(data.astype(np.uint8))\n",
    "    return new_im\n",
    "\n",
    "\n",
    "new_im = MatrixToImage(data)\n",
    "\n",
    "new_im.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Jephian:\n",
    "The question is asking you to use the model you trained in 1(a) to predict the digit you drew.  \n",
    "You may run 1(a) again and run the following code.  \n",
    "```python\n",
    "import os \n",
    "from PIL import Image\n",
    "path = \"DecisionTreeClassifier-with-scikit-learn-2021S-data\"\n",
    "filename = os.path.join(path, \"my_digit.png\")\n",
    "img = Image.open(filename).resize((8,8)).convert(\"L\")\n",
    "\n",
    "arr = np.array(img).ravel()\n",
    "arr = (255 - arr) * 16 / 255\n",
    "X_sample = arr[np.newaxis, :]\n",
    "model.predict(X_sample)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 4\n",
    "Let  \n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "```  \n",
    "and `model` be your decision tree classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4(a)\n",
    "Plot the decision tree with the keyword `node_ids=True` .  \n",
    "(If necessary, you may use `plt.figure(figsize=(15,15))` to change the figure size.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_new = model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plot_tree(model,node_ids=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4(b)\n",
    "Let `T = model.tree_` .  \n",
    "Print `T.children_left` and `T.children_right` .  \n",
    "Compare these two arrays with the decision tree you printed.  \n",
    "What do they mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "T = model.tree_\n",
    "print(T.children_left)\n",
    "print(T.children_right)\n",
    "#在left和right的第i個元素表示\n",
    "#在第i個node上，第i個node的左子節點和右子節點\n",
    "#-1代表沒有子節點"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4(c)\n",
    "Print `T.feature` and `T.threshold` .  \n",
    "Compare these two arrays with the decision tree you printed.  \n",
    "What do they mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "print(T.feature)\n",
    "#T.feature[i]表示\n",
    "#在分類第i個節點時，以第feature[i]個特徵進行分類\n",
    "print(T.threshold)\n",
    "#每個節點分類的臨界值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4(d)\n",
    "Print `T.n_node_samples` and `T.impurity` .  \n",
    "Compare these two arrays with the decision tree you printed.  \n",
    "What do they mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "print(T.n_node_samples)\n",
    "#T.n_node_samples[i]表示\n",
    "#在第i個節點中總共有T.n_node_samples[i]個樣本\n",
    "print(T.impurity)\n",
    "#值越大代表該節點所含的資訊量越雜\n",
    "#要再進一步找出feature進行分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4(e)\n",
    "For each `i = 0,1,2,3`, count how many nodes uses feature `i` for splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "for i in range(4):\n",
    "    print(\"Feature \",i,\" uses \",sum(T.feature==i),\" times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4(f)\n",
    "Suppose there are $N$ sample points in the training data.  \n",
    "Suppose a node contains $n_s$ sample points.  \n",
    "Within these sample points, there is a chance of $p_i$ to get class $i$.  \n",
    "One may calculate the impurity $H$ (Gini or entropy) at this point.  \n",
    "\n",
    "Suppose the \"information\" at each node is  \n",
    "$$I = \\frac{n_s}{N}\\cdot H.$$\n",
    "Calculate the information at each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### your answer here\n",
    "ns = T.n_node_samples\n",
    "H = T.impurity\n",
    "N=105\n",
    "I = (ns/N)*H\n",
    "for i in range(13):\n",
    "    print(\"The information of node \",i,\"is \",I[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4(e)\n",
    "Suppose $I$ is the information at one node, while $I_\\ell$ and $I_r$ are the information at its left and right children, respectively.  \n",
    "The **information gain** at this node is $I_\\ell + I_r -I$.  \n",
    "Calculate the information gain at each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "inf_g = np.zeros_like(I)\n",
    "for i in range(13):\n",
    "    left = T.children_left[i]\n",
    "    right = T.children_right[i]\n",
    "    if left == -1:\n",
    "        if right == -1:\n",
    "            inf_g[i] = I[i]\n",
    "        inf_g[i] = I[right]-I[i]\n",
    "    inf_g[i] = I[left]+I[right]-I[i]\n",
    "    print(\"The information gain of node \",i,\" is: \",inf_g[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4(f)\n",
    "Let $W_i$ be the sum of information gain among nodes using feature $i$ for splitting.  \n",
    "Calculate an array `W` such whose entries are `W_i` for each feature $i$.  \n",
    "Let `W = W / W.sum()` .  \n",
    "Compare `W` with `model.feature_importances_` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "feature = T.feature\n",
    "W = np.zeros(4)\n",
    "\n",
    "where0 = np.where(feature==0)\n",
    "where1 = np.where(feature==1)\n",
    "where2 = np.where(feature==2)\n",
    "where3 = np.where(feature==3)\n",
    "\n",
    "for i in range(4):\n",
    "    inf_where = np.where(feature==i)\n",
    "    W[i] = sum(inf_g[inf_where])\n",
    "\n",
    "W = W / W.sum()\n",
    "print(model.feature_importances_)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 5\n",
    "Let  \n",
    "```python \n",
    "X = 5 * np.random.randn(1000,2)\n",
    "lengths = np.linalg.norm(X, axis=1)\n",
    "band1 = (lengths > 1) & (lengths <2)  \n",
    "band2 = (lengths > 3) & (lengths <4)  \n",
    "X = X[band1 | band2, :]\n",
    "y = np.array([0]*band1.shape[0] + [1]*band2.shape[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5(a)\n",
    "Go through the split-train-test process.  \n",
    "What is the accuracy score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = 5 * np.random.randn(1000,2)\n",
    "lengths = np.linalg.norm(X, axis=1)\n",
    "band1 = (lengths > 1) & (lengths <2)  \n",
    "band2 = (lengths > 3) & (lengths <4)\n",
    "X = np.vstack([X[band1], X[band2]])\n",
    "y = np.array([0]*band1.sum() + [1]*band2.sum())\n",
    "\n",
    "score = np.array([])\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X, y)\n",
    "    y_new = model.predict(X_test)\n",
    "    \n",
    "    scr = np.sum(y_new==y_test)/y_test.shape[0]\n",
    "    score = np.append(score,scr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"test\",i+1,\": score =\",score[i])\n",
    "print(\"average score =\",np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5(b)\n",
    "Use some random points to plot the regions for each class.  \n",
    "(Just as what we did in Exercise 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "X_test=np.random.rand(1000,2)*10-5\n",
    "y_new = model.predict(X_test)\n",
    "plt.axis('equal')\n",
    "plt.scatter(X_test.T[0],X_test.T[1],c=y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
