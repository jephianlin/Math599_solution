{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Creative Commons License](https://i.creativecommons.org/l/by/4.0/88x31.png)  \n",
    "This work by Jephian Lin is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "**Input:**  \n",
    "- `X`: an array of shape `(N,d)` whose rows are samples and columns are features\n",
    "- `r`: target dimension\n",
    "\n",
    "**Output:**\n",
    "- an array of shape `(N, r)`  \n",
    "\n",
    "**Steps:**\n",
    "1. Shift $X$ so that the rows of $X$ are centered at the origin.  \n",
    "2. Let $C = \\frac{1}{N}X^\\top X$.  \n",
    "3. Suppose ${\\bf u}_1,\\ldots, {\\bf u}_d$ form an orthonormal eigenbasis of $C$ corresponding to the eigenvalues $\\lambda_1\\geq\\cdots\\geq\\lambda_d$.  Let $U$ be the matrix whose columns are ${\\bf u}_1,\\ldots, {\\bf u}_r$.  \n",
    "4. Return $XU$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode\n",
    "Translate the algorithm into the pseudocode.  \n",
    "This helps you to identify the parts that you don't know how to do it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. X = X - X.mean(axis=0)\n",
    "    2. N,d = X.shape\n",
    "       C = (X.T.dot(X))/N\n",
    "    3. vals,vecs = np.linalg.eigh(C)\n",
    "       vals = vals[::-1]\n",
    "       vecs = vecs[:,::-1]\n",
    "       U = vecs[:,:r]\n",
    "    4. return X@U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCAscratch(X,r): # X is a set of given data, r is our target dimension.\n",
    "    X = X - X.mean(axis=0) # Shift X so that the rows of X are centered at the origin.\n",
    "    N,d = X.shape # X is an array of shape (N,d), and we need the number of N to calculate C.\n",
    "    C = (X.T.dot(X))/N # C is a convariance matrix.\n",
    "    vals,vecs = np.linalg.eigh(C) # Get the eigenvalues and eigenvectors of C.\n",
    "    vals = vals[::-1] # Make eigenvalues from large to small.\n",
    "    vecs = vecs[:,::-1] # Make eigenvectors from large to small.\n",
    "    U = vecs[:,:r] # U only need the first r columns of vecs matrix.\n",
    "    return X@U # Return XU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "Take some sample data from [PCA-with-scikit-learn](PCA-with-scikit-learn.ipynb) and check if your code generates similar outputs with the existing packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Name of the data\n",
    "X = np.genfromtxt('hidden_text.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('hidden_text.csv', delimiter=',') \n",
    "# I also tried the load_digits, but just too difficult to check the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### results with your code\n",
    "print(PCAscratch(X,2))\n",
    "\n",
    "%matplotlib inline\n",
    "plt.axis('equal')\n",
    "plt.scatter(*PCAscratch(X,2).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### with scikit learn\n",
    "from sklearn.decomposition import PCA\n",
    "model = PCA(2)\n",
    "X_new = model.fit_transform(X)\n",
    "print(X_new)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.axis('equal')\n",
    "plt.scatter(*X_new.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check : 兩者差別就是上下顛倒，所以他們是很相似的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 1\n",
    "The center of rows of $X$ (before shift) is supposed to be `model.mean_` .  \n",
    "Check if this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('hidden_text.csv', delimiter=',')\n",
    "\n",
    "r = 2\n",
    "X = X - X.mean(axis=0) # Shift X so that the rows of X are centered at the origin.\n",
    "N,d = X.shape # X is an array of shape (N,d), and we need the number of N to calculate C.\n",
    "C = (X.T.dot(X))/N # C is a convariance matrix.\n",
    "vals,vecs = np.linalg.eigh(C) # Get the eigenvalues and eigenvectors of C.\n",
    "vals = vals[::-1] # Make eigenvalues from large to small.\n",
    "vecs = vecs[:,::-1] # Make eigenvectors from large to small.\n",
    "U = vecs[:,:r] # U only need the first r columns of vecs matrix.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "model = PCA(r)\n",
    "X_new = model.fit_transform(X)\n",
    "\n",
    "Ans = np.allclose(X.mean(axis=0),model.mean_) \n",
    "# If center of rows of X (before shift) and model.mean_ are same (very close), Ans will equal True.\n",
    "print(Ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 2\n",
    "The matrix $U^\\top$ is supposed to be `model.components_` .  \n",
    "(Up to some negations.)  \n",
    "Check if this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('hidden_text.csv', delimiter=',')\n",
    "\n",
    "r = 2\n",
    "X = X - X.mean(axis=0) # Shift X so that the rows of X are centered at the origin.\n",
    "N,d = X.shape # X is an array of shape (N,d), and we need the number of N to calculate C.\n",
    "C = (X.T.dot(X))/N # C is a convariance matrix.\n",
    "vals,vecs = np.linalg.eigh(C) # Get the eigenvalues and eigenvectors of C.\n",
    "vals = vals[::-1] # Make eigenvalues from large to small.\n",
    "vecs = vecs[:,::-1] # Make eigenvectors from large to small.\n",
    "U = vecs[:,:r] # U only need the first r columns of vecs matrix.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "model = PCA(r)\n",
    "X_new = model.fit_transform(X)\n",
    "\n",
    "Ans = True # Suppose Ans is True at first.\n",
    "# 因為每個主成分(向量)可以是負的也可以是正的，只要是同一條線上就好了。\n",
    "# 所以我們在做比較的時候，要把乘上一個負號就相等的情況也考慮進去。 (這段不太會用英文講)\n",
    "for i in range(r) :\n",
    "    if np.allclose(U.T[i],model.components_[i])| np.allclose(-U.T[i],model.components_[i]) == False :\n",
    "        Ans = False # 如果有任何一個主成分 (U.T的橫列) 不符合上述情況，就代表UT不是model.components_，把Ans改False然後break迴圈。\n",
    "        break\n",
    "# 如果跑完迴圈 (比較完每個主成分) Ans仍為True，我們就說他們是相等的 (很靠近)。\n",
    "print(Ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3\n",
    "The sequence $\\lambda_1,\\ldots,\\lambda_r$ are suppose to be the values in `explained_variance_` .  \n",
    "Check if this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('hidden_text.csv', delimiter=',')\n",
    "\n",
    "r = 2\n",
    "X = X - X.mean(axis=0) # Shift X so that the rows of X are centered at the origin.\n",
    "N,d = X.shape # X is an array of shape (N,d), and we need the number of N to calculate C.\n",
    "C = (X.T.dot(X))/N # C is a convariance matrix.\n",
    "vals,vecs = np.linalg.eigh(C) # Get the eigenvalues and eigenvectors of C.\n",
    "vals = vals[::-1] # Make eigenvalues from large to small.\n",
    "vecs = vecs[:,::-1] # Make eigenvectors from large to small.\n",
    "U = vecs[:,:r] # U only need the first r columns of vecs matrix.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "model = PCA(r)\n",
    "X_new = model.fit_transform(X)\n",
    "\n",
    "print(np.allclose(vals[:r],model.explained_variance_)) \n",
    "# Their errors are bigger than model.explained_variance_*rtol (rtol = 1.e-5), so we cannot say they are same under rtol = 1.e-5.\n",
    "print(model.explained_variance_) # Observe the value of model.explained_variance_.\n",
    "print(vals[:r]) # Observe the value of vals[:r].\n",
    "# I think this two arrays are very close, so I try to  adjust the rtol of np.allclose function.\n",
    "Ans = np.allclose(vals[:r],model.explained_variance_,rtol = 1.e-3)\n",
    "print(Ans)\n",
    "# Hence, under the rtol = 1.e-3, we can say they are same (very close)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 4\n",
    "Let $t = \\operatorname{tr}(C)$.  \n",
    "The sequence $\\lambda_1/t,\\ldots,\\lambda_r/t$ are suppose to be the values in `explained_variance_ratio_` .  \n",
    "Check if this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('hidden_text.csv', delimiter=',')\n",
    "\n",
    "r = 2\n",
    "X = X - X.mean(axis=0) # Shift X so that the rows of X are centered at the origin.\n",
    "N,d = X.shape # X is an array of shape (N,d), and we need the number of N to calculate C.\n",
    "C = (X.T.dot(X))/N # C is a convariance matrix.\n",
    "vals,vecs = np.linalg.eigh(C) # Get the eigenvalues and eigenvectors of C.\n",
    "vals = vals[::-1] # Make eigenvalues from large to small.\n",
    "vecs = vecs[:,::-1] # Make eigenvectors from large to small.\n",
    "U = vecs[:,:r] # U only need the first r columns of vecs matrix.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "model = PCA(r)\n",
    "X_new = model.fit_transform(X)\n",
    "\n",
    "t = np.trace(C)\n",
    "Ans = np.allclose(vals[:r]/t,model.explained_variance_ratio_) \n",
    "# If the sequence 𝜆1/𝑡,…,𝜆𝑟/𝑡 and explained_variance_ratio_ are same (very close), Ans will equal True.\n",
    "print(Ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 5\n",
    "The singular values of the shifted `X` are supposed to be `model.singular_values_` .  \n",
    "Check if this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('hidden_text.csv', delimiter=',')\n",
    "\n",
    "r = 2\n",
    "X = X - X.mean(axis=0) # Shift X so that the rows of X are centered at the origin.\n",
    "N,d = X.shape # X is an array of shape (N,d), and we need the number of N to calculate C.\n",
    "C = (X.T.dot(X))/N # C is a convariance matrix.\n",
    "vals,vecs = np.linalg.eigh(C) # Get the eigenvalues and eigenvectors of C.\n",
    "vals = vals[::-1] # Make eigenvalues from large to small.\n",
    "vecs = vecs[:,::-1] # Make eigenvectors from large to small.\n",
    "U = vecs[:,:r] # U only need the first r columns of vecs matrix.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "model = PCA(r)\n",
    "X_new = model.fit_transform(X)\n",
    "\n",
    "u,s,v = np.linalg.svd(X) # s is an array of singular values (from large to small).\n",
    "Ans = np.allclose(s[:r],model.singular_values_) \n",
    "# If first r of singular values of the shifted X and model.singular_values_ are same (very close), Ans will equal True.\n",
    "print(Ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
