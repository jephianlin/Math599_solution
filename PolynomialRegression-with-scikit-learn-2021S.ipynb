{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6EG_0Q4zXvY"
   },
   "source": [
    "# PolynomialRegression with scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptCX9EREzXvc"
   },
   "source": [
    "![Creative Commons License](https://i.creativecommons.org/l/by/4.0/88x31.png)  \n",
    "This work by Jephian Lin is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MTOUkfEzXvd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    " \n",
    "def PolynomialRegression(degree=2, fit_intercept=True):\n",
    "    return make_pipeline(PolynomialFeatures(degree=degree, include_bias=False), \n",
    "                         LinearRegression(fit_intercept=fit_intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtnhnmDOzXve"
   },
   "source": [
    "## Code\n",
    "```python\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def PolynomialRegression(degree=2, fit_intercept=True):\n",
    "    return make_pipeline(PolynomialFeatures(degree=degree, include_bias=False), \n",
    "                         LinearRegression(fit_intercept=fit_intercept))\n",
    "```\n",
    "\n",
    "Official Reference: [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) and [make_pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC7n04FmzXvf"
   },
   "source": [
    "## Parameters\n",
    "- `degree`: the degree of the polynomial  \n",
    "for example, if `degree=2` and two features a and b are given  \n",
    "then a, b, a^2, a*b, b^2 are generated as the expanded features\n",
    "- `fit_intercept`: whether to calculate the intercept or not  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrbZDfRAzXvf"
   },
   "source": [
    "## Attributes\n",
    "For `model[0]`:\n",
    "- `n_input_features`: the width of `X`  \n",
    "- `n_output_features`: the number of expanded features\n",
    "- `powers_` : an array of shape (n_output_features, n_input_features) that stores how each expanded feature is obtained\n",
    "\n",
    "For `model[1]`:\n",
    "- `coef_`: an array of shape `(n_output_features,)` that stores the coefficients for each expanded feature\n",
    "- `intercept_`: the coefficient of the constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_x4uiHSzXvg"
   },
   "source": [
    "## Sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCSyQ2UmzXvg"
   },
   "source": [
    "##### Exercise 1\n",
    "Let  \n",
    "```python\n",
    "x = np.arange(10)\n",
    "y = 0.1*x**2 + 0.2*x + 0.3 + 0.5*np.random.randn(10)\n",
    "X = x[:,np.newaxis]\n",
    "x_test = np.linspace(0,10,20)\n",
    "X_test = x_test[:,np.newaxis]\n",
    "\n",
    "model = PolynomialRegression(2)\n",
    "model.fit(X, y)\n",
    "y_new = model.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QhAV9y0zXvh"
   },
   "source": [
    "###### 1(a)\n",
    "Use `plt.scatter` to plot the points with `x` and `y` .  \n",
    "Use `plt.plot( ..., c='r')` to plot the line with `x_test` and `y_new` .  \n",
    "Print `model.coef_` and  `model.intercept_` .  \n",
    "Can you guess these values by the definition of `y` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "h_l-tDbFzXvh",
    "outputId": "3342b4da-ecb7-4375-f8a5-c0fb2b764089"
   },
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "y = 0.1*x**2 + 0.2*x + 0.3 + 0.5*np.random.randn(10)\n",
    "X = x[:,np.newaxis]\n",
    "x_test = np.linspace(0,10,20)\n",
    "X_test = x_test[:,np.newaxis]\n",
    " \n",
    "model = PolynomialRegression(2)\n",
    "model.fit(X, y)\n",
    "y_new = model.predict(X_test)\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x_test,y_new, c = \"r\")\n",
    "print(model[1].coef_)\n",
    "print(model[1].intercept_)\n",
    "print(\"Yes, these values can be guessed by definition of y because y is defined in almost perfect polynomial formula\")\n",
    "print(\"Without considering added random noise: coefficient with second power should be around 0.1, linear coefficient 0.2 and intercept 0.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhhK6mCXzXvi"
   },
   "source": [
    "###### 1(b)\n",
    "Redo 1(a) with the setting `fit_intercept=False` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "OOkJxX9jzXvi",
    "outputId": "7d2c67a0-7c20-4caf-dab6-e7405c62b008"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = np.arange(10)\n",
    "y = 0.1*x**2 + 0.2*x + 0.3 + 0.5*np.random.randn(10)\n",
    "X = x[:,np.newaxis]\n",
    "x_test = np.linspace(0,10,20)\n",
    "X_test = x_test[:,np.newaxis]\n",
    "\n",
    "model = PolynomialRegression(2,fit_intercept=False) \n",
    "print(\"If set it_intercept=False, no intercept will be used in calculations (i.e. data is expected to be centered)\")\n",
    "model.fit(X, y)\n",
    "y_new = model.predict(X_test)\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x_test,y_new,c='r')\n",
    "print(model[1].coef_)\n",
    "print(model[1].intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSEycxXVzXvi"
   },
   "source": [
    "##### Exercise 2\n",
    "Let  \n",
    "```python\n",
    "x1 = np.arange(5)\n",
    "X = np.vstack([x1]).T\n",
    "\n",
    "model = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_ex = model.fit_transform(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_fXCs0bzXvj"
   },
   "source": [
    "###### 2(a)\n",
    "Understand the relation between `X` and `X_ex` .  \n",
    "Can you generate `X_ex` by boradcasting instead of the `PolynomialFeatures` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oxv5ORHCzXvj",
    "outputId": "7de28a7c-7baa-4e93-883a-2e13cebc72b6"
   },
   "outputs": [],
   "source": [
    "x1 = np.arange(5)\n",
    "X = np.vstack([x1]).T\n",
    "\n",
    "model = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_ex = model.fit_transform(X)\n",
    "\n",
    "print(\"X_ex=\\n\",X_ex)\n",
    "print(\"\\nX_ex just contains expanded features calculated as 1st,2nd and 3rd power of features in X vector\\n\")\n",
    "print(np.hstack((X**1, X**2, X**3)))\n",
    "#X_ex = X,X**2,X**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9ow43sWzXvj"
   },
   "source": [
    "###### 2(b)\n",
    "Switch the setting to `include_bias=True` .  \n",
    "Understand the relation between `X` and `X_ex` .  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qI-t2zJIzXvk",
    "outputId": "adf4eb0f-db0e-4225-9e82-4acb3fc8e669"
   },
   "outputs": [],
   "source": [
    "x1 = np.arange(5)\n",
    "X = np.vstack([x1]).T\n",
    "\n",
    "model = PolynomialFeatures(degree=3, include_bias=True)\n",
    "X_ex = model.fit_transform(X)\n",
    "\n",
    "print(\"X_ex=\\n\",X_ex)\n",
    "print(\"\\nX_ex is the same as before but additionally it also contains a bias column\")\n",
    "print(\"the feature in which all polynomial powers are zero - acts as an intercept term in a linear model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohmE7K6GzXvk"
   },
   "source": [
    "###### 2(c)\n",
    "Let  \n",
    "```python\n",
    "x1 = np.arange(5)\n",
    "x2 = np.arange(5,10)\n",
    "X = np.vstack([x1,x2]).T\n",
    "\n",
    "model = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_ex = model.fit_transform(X)\n",
    "```\n",
    "Print `model.powers_` and understand the relation between `X` and `X_ex` .  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHd5LYlyzXvk",
    "outputId": "e52158d1-33ec-4844-eb4f-1e5f7948d678"
   },
   "outputs": [],
   "source": [
    "\n",
    "x1 = np.arange(5)\n",
    "x2 = np.arange(5,10)\n",
    "X = np.vstack([x1,x2]).T\n",
    " \n",
    "model = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_ex = model.fit_transform(X)\n",
    "print(X)\n",
    "print(X_ex)\n",
    "print(model.powers_)\n",
    "print(\"It's the same as:\\n(x1^1 * x2^0)\\n(x1^0 * x2^1)\\n(x1^2 * x2^0)\\n(x1^1 * x2^1)\\n(x1^0 * x2^2)\\n in transform\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ja6uGeHAzXvl"
   },
   "source": [
    "###Exercise 3\n",
    "Let\n",
    "###r = 100 * np.random.rand(100)\n",
    "###area = 4*np.pi*r**2 + 0.5*np.random.randn(100)\n",
    "be a collection of data of 100 balls,\n",
    "where c stores the radii and\n",
    "\n",
    "area stores the surface areas.\n",
    "\n",
    "Suppose you knows nothing about the formula of the surface area of a sphere.\n",
    "\n",
    "How would you guess their relation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "N-F5c7gmzXvl",
    "outputId": "03a02dec-8794-4d70-e3b1-7d843bdafc56"
   },
   "outputs": [],
   "source": [
    "r = 100 * np.random.rand(100)\n",
    "area = 4*np.pi*r**2 + 0.5*np.random.randn(100)\n",
    "\n",
    "R = r[:,np.newaxis]\n",
    "r_test = np.linspace(0,100,30)\n",
    "R_test = r_test[:,np.newaxis]\n",
    "\n",
    "model = PolynomialRegression(2) \n",
    "model.fit(R, area)\n",
    "area_test = model.predict(R_test)\n",
    "plt.scatter(r,area)\n",
    "plt.plot(R_test,area_test,c='r')\n",
    "print(model[1].coef_)\n",
    "print(model[1].intercept_)\n",
    "print(\"area=intercept+coef[0]r+coef[2]r^2\")\n",
    "print(\"Degree=1 (X), degree=2,3,4 the results look quite good => choose the smallest suitable degree=2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-of2LVxBzXvm"
   },
   "source": [
    "##### Exercise 4\n",
    "Let  \n",
    "```python\n",
    "r = 100 * np.random.rand(100)\n",
    "volume = 4/3*np.pi*r**3 + 0.5*np.random.randn(100)\n",
    "```\n",
    "be a collection of data of 100 balls,  \n",
    "where `c` stores the radii and  \n",
    "`volume` stores the volumes.  \n",
    "Suppose you knows nothing about the formula of the surface area of a sphere.  \n",
    "How would you guess their relation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "Jtgbed6mzXvm",
    "outputId": "d7e3c113-f5a4-41b2-b17b-321b12eb5b37"
   },
   "outputs": [],
   "source": [
    "r = 100 * np.random.rand(100)\n",
    "R = r[:,np.newaxis]\n",
    "volume = 4/3*np.pi*r**3 + 0.5*np.random.randn(100)\n",
    "plt.scatter(r, volume, c='r')\n",
    "\n",
    "model = PolynomialRegression(3)\n",
    "model.fit(R, volume)\n",
    "r_test = np.linspace(0,100,50)[:,np.newaxis]\n",
    "volume_test = model.predict(r_test)\n",
    "plt.plot(r_test, volume_test)\n",
    "\n",
    "print(\"area=intercept+coef[0]r+coef[2]r^2\")\n",
    "print(\"Without knowing the formula we can try multiple PolynomialRegression models with various degrees\")\n",
    "print(\"The smallest degree exactly fitting our radius and volume data is 3. That's why we can assume that the relation is cubic.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qztMksFzXvm"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zExxZDcVzXvn"
   },
   "source": [
    "##### Exercise 5\n",
    "Let  \n",
    "```python\n",
    "x = np.arange(10)\n",
    "y = 0.1*x**2 + 0.2*x + 0.3 + 0.5*np.random.randn(10)\n",
    "X = x[:,np.newaxis]\n",
    "x_test = np.linspace(0,10,20)\n",
    "X_test = x_test[:,np.newaxis]\n",
    "```\n",
    "For `k = 0, ..., 4`, run the polynomial regression model with `degree=k`.  \n",
    "Let `scores` be a list storing their scores.  \n",
    "Plot the scores.  \n",
    "Which degree is an appropriate guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "YhgWZZu8zXvn",
    "outputId": "3288eaf6-8ee1-4675-cb72-6195d4adad58"
   },
   "outputs": [],
   "source": [
    "#博勛的方法\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "SIZE=10\n",
    "x = np.arange(SIZE)\n",
    "y = 0.1*x**2 + 0.2*x + 0.3 + 0.5*np.random.randn(SIZE)\n",
    "X = x[:,np.newaxis]\n",
    "\n",
    "training_size = int(SIZE*0.8)\n",
    "testing_size = SIZE-training_size\n",
    "indices = np.random.permutation(x.shape[0])\n",
    "training_idx, test_idx = indices[:training_size], indices[testing_size:]\n",
    "\n",
    "training_x, training_y = x[training_idx][:,np.newaxis], y[training_idx]\n",
    "test_x, test_y = x[test_idx][:,np.newaxis], y[test_idx]\n",
    "\n",
    "UP_TO_K_DEGREE = 5\n",
    "msqs = []\n",
    "ks = np.arange(1, UP_TO_K_DEGREE)\n",
    "for k in ks:\n",
    "    model = PolynomialRegression(k)\n",
    "    model.fit(training_x, training_y)\n",
    "    \n",
    "    predict_y = model.predict(test_x)\n",
    "    msqs.append(mean_squared_error(test_y, predict_y))\n",
    "    \n",
    "msqs = np.array(msqs)\n",
    "plt.scatter(ks, msqs)\n",
    "plt.xlabel(\"polynomial degree of model\")\n",
    "plt.ylabel(\"mean squared error\")\n",
    "\n",
    "print(\"Based on calculation of y, proper degree should be 2. However, in my opinion, using only 10 data points is not enough to correctly guess the degree \")\n",
    "print(\"since the noise can be too big. In following plot we can see mean squared error of each model at y axis with its respective degree at x axis. \")\n",
    "print(\"If we run the cell multiple times, we see big changes.\")\n",
    "print(\"By adjusting (SIZE, UP_TO_K_DEGREE) variables to for example (100, 50) we can see that degree 2 is enough \")\n",
    "print(\"and from around degree 20 we start overfitting the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "9aqlwZm6ZB-8",
    "outputId": "6e7bc171-ff84-422c-f9d8-ac0ec5006f78"
   },
   "outputs": [],
   "source": [
    "#Sanny的方法\n",
    "from sklearn.metrics import mean_squared_error\n",
    "x = np.arange(10)\n",
    "X = x[:,np.newaxis]\n",
    "y = 0.1*x**2 + 0.2*x + 0.3 + 0.5*np.random.randn(10)\n",
    "\n",
    "scores=[] #Build a list storing scores\n",
    "degree= np.arange(1,5)\n",
    "for k in degree:\n",
    "    model = PolynomialRegression(k)\n",
    "    model.fit(X, y)\n",
    "    y_new = model.predict(X) \n",
    "    scores.append(mean_squared_error(y_new,y)) \n",
    "scores=np.array(scores)\n",
    "\n",
    "plt.scatter(degree,scores)\n",
    "print(scores)\n",
    "\n",
    "print(\"Choose Degree=2 (The smallest suitable degree\")\n",
    "print(\"Since there are only 10 data points, it is not necessary to split the data here.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0klIlYZzXvo"
   },
   "source": [
    "##### Exercise 6\n",
    "Let  \n",
    "```python\n",
    "x = np.arange(10)\n",
    "y = 0.1*x**2 + 0.2*x + 0.3 + 0.5*np.random.randn(10)\n",
    "X = x[:,np.newaxis]\n",
    "\n",
    "model = PolynomialRegression(2)\n",
    "model.fit(X, y)\n",
    "y_new = model.predict(X)\n",
    "\n",
    "a0 = model[1].intercept_\n",
    "a1,a2 = model[1].coef_\n",
    "```\n",
    "The prediction `y_new` is supposed to be the same as `a0 + a1*x + a2*x**2` .  \n",
    "Check if it is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gsg-m5BhzXvo",
    "outputId": "4ab23117-7fd5-4346-e38b-d60a6393b016"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = np.arange(10)\n",
    "y = 0.1*x**2 + 0.2*x + 0.3 + 0.5*np.random.randn(10)\n",
    "X = x[:,np.newaxis]\n",
    " \n",
    "model = PolynomialRegression(2)\n",
    "model.fit(X, y)\n",
    "y_new = model.predict(X)\n",
    " \n",
    "a0 = model[1].intercept_\n",
    "a1,a2 = model[1].coef_\n",
    "print(y_new-(a0 + a1*x+a2*x**2))\n",
    "print(\"We can see that is the statement is merely the same\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "PolynomialRegression-with-scikit-learn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
