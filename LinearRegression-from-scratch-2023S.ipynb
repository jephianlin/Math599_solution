{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3iMHnvU34zW"
   },
   "source": [
    "# LinearRegression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ERFhL7A34za"
   },
   "source": [
    "![Creative Commons License](https://i.creativecommons.org/l/by/4.0/88x31.png)  \n",
    "This work by Jephian Lin is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aly2C7Dh34za"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMSRD0Eq34zc"
   },
   "source": [
    "## Algorithm\n",
    "**Input:**  \n",
    "- `X`: an array of shape `(N,d)` whose rows are samples and columns are features\n",
    "- `y`: the labels of shape `(N,)`\n",
    "- `fit_intercept`: whether to calculate the intercept or not  \n",
    "- `algorithm`: `\"projction\"` or `\"grad_descent\"`\n",
    "- `learning_rate`: learning rate for the gradient descent algorithm\n",
    "- `n_iter`: number of iterations for the gradient descent algorithm \n",
    "\n",
    "**Output:**  \n",
    "A tuple `(predict, coefs, intercep)`.    \n",
    "- `predict`: a function that can takes some samples `X_test` and return the prediction `X_test.dot(coefs) + intercept` \n",
    "- `coef`: an array of shape `(d,)` that stores the coefficients\n",
    "- `intercept`: a float for the intercept\n",
    "\n",
    "**Steps:**\n",
    "1. If `fit_intercept`, let $A$ be the matrix obtained from $X$ by adding a column of ones on the left; otherwise, let $A = X$ (make a copy).  Let `dp` be the number of columns of $A$.\n",
    "2. If `algorithm==\"projection\"`, compute ${\\bf v} = (A^\\top A)^{-1}A^\\top {\\bf y}$.\n",
    "3. If `algorithm==\"grad_descent\"`, run the gradient descent algorithm as follows:\n",
    "    1. Pick a random vector ${\\bf v}$ of shape `(dp,)` .\n",
    "    2. Calculate the gradient $\\nabla = \\frac{2}{N}(A{\\bf v} - {\\bf y})^\\top A$.\n",
    "    3. Update ${\\bf v}$ by ${\\bf v} - \\alpha\\nabla$.\n",
    "    4. Repeat Steps B and C `n_iter` times.\n",
    "4. If `fit_intercept`, let `coef` be ${\\bf v}[1:]$ and `intercept` be ${\\bf v}[0]$; otherwise, let `coef` be ${\\bf v}$ and `intercept` be 0.  \n",
    "5. Define `predict` as a function that sends `X_test` to `X_test.dot(coefs) + intercept`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSKZNqLW34zc"
   },
   "source": [
    "## Pseudocode\n",
    "Translate the algorithm into the pseudocode.  \n",
    "This helps you to identify the parts that you don't know how to do it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJl-rLqM34zd"
   },
   "source": [
    "    1. Pseudocode: Migel\n",
    "    2. Exercise 1: 吳昀哲 \n",
    "    3. Exercise 2: 陸昱儒\n",
    "    4. Exercise 3: 楊芷綺"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BS7RMrY34zd"
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FaN-KyTc34zd"
   },
   "outputs": [],
   "source": [
    "### your answer here\n",
    "def LR(X, y, fit_intercept=True, algorithm=\"projection\", learning_rate=.001, n_iter=100):\n",
    "    N,d = X.shape\n",
    "\n",
    "    if fit_intercept:\n",
    "        A = np.hstack([np.ones((N,1)), X])\n",
    "        d += 1\n",
    "    else:\n",
    "        A = X.copy()\n",
    "        \n",
    "    if algorithm == \"projection\":\n",
    "        v = np.linalg.inv(A.T.dot(A)).dot(A.T).dot(y)\n",
    "    elif algorithm == \"grad_descent\":\n",
    "        v = np.random.randn(d)\n",
    "        for i in range(n_iter):\n",
    "            grad = (2/N) * ((A.dot(v)-y).T).dot(A) \n",
    "            A -= learning_rate * grad\n",
    "\n",
    "    if fit_intercept:\n",
    "        coef = v[1:]\n",
    "        intercept = v[0]\n",
    "    else:\n",
    "        coef = v.copy()\n",
    "        intercept = 0\n",
    "\n",
    "    predict = lambda X_test: X_test.dot(coef) + intercept\n",
    "\n",
    "    return coef, intercept, predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Veronica\n",
    "\n",
    "`A` should be changed into `v`.\n",
    "```python\n",
    " elif algorithm == \"grad_descent\":\n",
    "        v = np.random.randn(d)\n",
    "        for i in range(n_iter):\n",
    "            grad = (2/N) * ((A.dot(v)-y).T).dot(A) \n",
    "            v -= learning_rate * grad\n",
    "```\n",
    "And, the order of the results shoule be changed into the following order since your output in the question 2(a) is `predict, coef, intercept`.\n",
    "\n",
    "```python\n",
    "  return predict, coef, intercept\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOrrx2iA34ze"
   },
   "source": [
    "## Test\n",
    "Take some sample data from [LinearRegression-with-scikit-learn](LinearRegression-with-scikit-learn.ipynb) and check if your code generates similar outputs with the existing packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAqh2kHU34ze"
   },
   "source": [
    "##### Name of the data\n",
    "Description of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQcjBxX934ze"
   },
   "outputs": [],
   "source": [
    "### results with your code\n",
    "### results with your code\n",
    "\n",
    "\n",
    "x = np.arange(10)\n",
    "y = 0.5*x + 3 + 0.3*np.random.randn(10)\n",
    "X = x[:,np.newaxis]\n",
    "x_test = np.linspace(0,10,20)\n",
    "X_test = x_test[:,np.newaxis]\n",
    "\n",
    "coef, intercept, predict  = LR(X, y, algorithm=\"projection\")\n",
    "y_new = predict(X_test)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x_test, y_new, c='r')\n",
    "\n",
    "print('coef_=', coef)\n",
    "print('intercept_=', intercept) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UFc53Ay34zf"
   },
   "outputs": [],
   "source": [
    "### results with existing packages\n",
    "\n",
    "# x = np.arange(10) # (10,)\n",
    "# y = 0.5*x + 3 + 0.3*np.random.randn(10)\n",
    "# X = x[:,np.newaxis] # (10,1)\n",
    "# x_test = np.linspace(0,10,20)\n",
    "# X_test = x_test[:,np.newaxis] # (20,1)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_new = model.predict(X_test)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x_test, y_new, c='r')\n",
    "\n",
    "print('model.coef_=', model.coef_)   \n",
    "print('model.intercept_=', model.intercept_)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRkozUZy34zf"
   },
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTywsXJv34zf"
   },
   "source": [
    "##### Exercise 1\n",
    "Set `algorithm=\"projection\"` .  \n",
    "Let  \n",
    "```python\n",
    "x = np.arange(10)\n",
    "X1 = np.vstack([x]).T\n",
    "X2 = np.vstack([np.ones_like(x), x]).T\n",
    "y = 0.5 * x + 3 + 0.3*np.random.randn(10)\n",
    "```\n",
    "Apply your code to `X1` with `fit_intercept=True` and obtain `(predict1, coef1, intercept1)` .  \n",
    "Apply your code to `X2` with `fit_intercept=False` and obtain `(predict2, coef2, intercept2)` .  \n",
    "What are the relation between `coef1`, `intercept1` and `coef2` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "D_1c1dSA34zf",
    "outputId": "e9c8a2eb-481e-4e47-f64a-07ad01dbcc46"
   },
   "outputs": [],
   "source": [
    "### your answer here\n",
    "x = np.arange(10)\n",
    "X1 = np.vstack([x]).T\n",
    "X2 = np.vstack([np.ones_like(x), x]).T\n",
    "y = 0.5 * x + 3 + 0.3*np.random.randn(10)\n",
    "\n",
    "predict1, coef1, intercept1 = LR(X1, y, algorithm =\"projection\")\n",
    "print('coef1 = ',coef1)\n",
    "print('intercept1 = ',intercept1)\n",
    "\n",
    "predict2, coef2, intercept2 = LR(X2, y, algorithm=\"projection\", fit_intercept=False)\n",
    "print('coef2 = ',coef2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cnBXZJy34zg"
   },
   "source": [
    "##### Exercise 2\n",
    "Let  \n",
    "```python\n",
    "x = np.arange(10)\n",
    "X = np.vstack([x]).T\n",
    "y = 0.5 * x + 3 + 0.3*np.random.randn(10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBpv7XoQ34zg"
   },
   "source": [
    "###### 2(a)\n",
    "Apply the linear regresssion algorithm to `X`  \n",
    "1. by your code with `algorithm==\"projection\"` ,  \n",
    "2. by your code with `algorithm==\"grad_descent\"` ,  \n",
    "3. by `sklearn.linear_model.LinearRegresssion` .  \n",
    "Check if the outputs are almost the same (up to some numerical errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "mb6ZKsTV34zg",
    "outputId": "1897d4d9-4b20-4a03-fd36-497e2b0c4f4c"
   },
   "outputs": [],
   "source": [
    "### your answer here\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x = np.arange(10)\n",
    "X = np.vstack([x]).T\n",
    "y = 0.5 * x + 3 + 0.3*np.random.randn(10)\n",
    "\n",
    "predict, coef, intercept = LR(X, y, algorithm=\"projection\")\n",
    "print('projection method')\n",
    "print('coef = ',coef)\n",
    "print('intercept = ',intercept)\n",
    "\n",
    "predict, coef, intercept = LR(X, y, algorithm=\"grad_descent\")\n",
    "print('grad_descent method')\n",
    "print('coef = ',coef)\n",
    "print('intercept = ',intercept)\n",
    "\n",
    " \n",
    "model = LinearRegression()\n",
    "model.fit(X,y)\n",
    "print('sklearn.linear_model.LinearRegression')\n",
    "print('coef = ',model.coef_)\n",
    "print('intercept = ',model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Veronica\n",
    "\n",
    "From your result, you know that the output is different when the algorithm is `grad_descent`.   \n",
    "\n",
    "I suggest that your can change the value of `n_iter`.When you have the bigger value, it can train more times. The output will be closed to the other two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Av9EpZg034zg"
   },
   "source": [
    "###### 2(b)\n",
    "Change `learning_rate=0.1` .  \n",
    "What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "QBPJyCDv34zg",
    "outputId": "54a61b16-acab-4440-dadf-d770627fc759"
   },
   "outputs": [],
   "source": [
    "### your answer here\n",
    "\n",
    "predict, coef, intercept = LR(X, y, algorithm=\"grad_descent\", learning_rate=0.1)\n",
    "\n",
    "print(' Change learning_rate=0.1: ')\n",
    "print(coef)\n",
    "print(intercept)\n",
    "\n",
    "# It diverges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JlQ5GNk34zg"
   },
   "source": [
    "###### 2(c)\n",
    "Change `learning_rate=0.0001` .  \n",
    "What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "XWR5D_4434zg",
    "outputId": "3b4f1602-60d1-43ac-eb2a-3ee99e0addd9"
   },
   "outputs": [],
   "source": [
    "### your answer here\n",
    "\n",
    "predict, coef, intercept = LR(X, y, algorithm=\"grad_descent\", learning_rate=0.0001)\n",
    "\n",
    "print(' Change learning_rate=0.0001: ')\n",
    "print(coef)\n",
    "print(intercept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxuyI6Lo34zh"
   },
   "source": [
    "###### 2(d)\n",
    "Modify your code so that it prints the mean square error at each step of the gradient descent.  \n",
    "Check if it is always decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-2xUkv134zh",
    "outputId": "c109cd3f-6a52-40d8-b940-7d1e1ace74b7"
   },
   "outputs": [],
   "source": [
    "### your answer here\n",
    "\n",
    "def LR_gradient(X, y, fit_intercept=True, algorithm=\"projction\", learning_rate = 0.01, n_iter = 10):\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    \n",
    "    if fit_intercept:\n",
    "        A = np.hstack([np.ones((N,1)),X])\n",
    "        dp = d+1\n",
    "    else:\n",
    "        A = X.copy()\n",
    "        dp = d\n",
    "        \n",
    "    if algorithm == \"projection\":\n",
    "        v = np.linalg.inv(A.T.dot(A)).dot(A.T).dot(y)\n",
    "    elif algorithm == \"grad_descent\":\n",
    "        v = np.random.rand(dp,)\n",
    "        for i in range(n_iter):\n",
    "            grad = (2/N)*((A.dot(v)-y).T).dot(A)\n",
    "            v = v - learning_rate*grad\n",
    "            print(np.linalg.norm(A.dot(v)-y)**2/X.shape[0])\n",
    "            \n",
    "LR_gradient(X, y, algorithm = \"grad_descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Veronica  \n",
    "\n",
    "You know that the mean square error is always decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlCAHbnU34zh"
   },
   "source": [
    "##### Exercise 3\n",
    "This exercise checks if the gradient formula is correct (or at least reasonable).  \n",
    "Let  \n",
    "```python\n",
    "N,dp = 100,3\n",
    "np.random.seed(20025)\n",
    "A = np.random.randn(N,dp)\n",
    "v = np.random.randn(dp)\n",
    "y = np.random.randn(N)\n",
    "```\n",
    "Define $c(A, {\\bf v}, {\\bf y}) = \\frac{1}{N}\\|A{\\bf v} - {\\bf y}\\|^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcnwYdS-34zh"
   },
   "source": [
    "###### 3(a)\n",
    "Write a function `cost(A, v, y)` that calculate $c(A, {\\bf v}, {\\bf y})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfzxfylI34zh"
   },
   "outputs": [],
   "source": [
    "### your answer here\n",
    "N,dp = 100,3\n",
    "np.random.seed(20025)\n",
    "A = np.random.randn(N,dp)\n",
    "v = np.random.randn(dp)\n",
    "y = np.random.randn(N)\n",
    "\n",
    "def cost(A, v, y):\n",
    "    return (1/N)*(np.linalg.norm(A.dot(v)-y))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHeY7XEf34zh"
   },
   "source": [
    "###### 3(b)\n",
    "Calculate the gradient  \n",
    "$$\\frac{2}{N}(A{\\bf v} - y)^\\top A.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6OKGYUG34zh",
    "outputId": "34d89a33-458e-4263-b8d1-0731ef7235b0"
   },
   "outputs": [],
   "source": [
    "### your answer here\n",
    "2/N*(A.dot(v)-y).T.dot(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvii8E5r34zh"
   },
   "source": [
    "###### 3(c)\n",
    "Let  \n",
    "```python\n",
    "h = 0.001\n",
    "i = 0\n",
    "e = np.zeros((dp,))\n",
    "e[i] = 1\n",
    "g = (cost(A, v+h*e, y) - cost(A, v, y)) / h\n",
    "```\n",
    "Run the code for `i = 0,1,2` and compare `g` with the gradient in 3(b).  \n",
    "If necessary, change `h` to smaller numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S21K3Yue34zh",
    "outputId": "11a5dc31-65b4-409e-cb2c-9ff621b505df"
   },
   "outputs": [],
   "source": [
    "### your answer here\n",
    "h = 0.00000001\n",
    "i = 0\n",
    "\n",
    "for i in range(3):\n",
    "    e = np.zeros((dp,))\n",
    "    e[i] = 1\n",
    "    g = (cost(A, v+h*e, y) - cost(A, v, y)) / h\n",
    "    print(g)\n",
    "    \n",
    "# g is similar to the gradient in 3(b)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
