{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearRegression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Creative Commons License](https://i.creativecommons.org/l/by/4.0/88x31.png)  \n",
    "This work by Jephian Lin is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "**Input:**  \n",
    "- `X`: an array of shape `(N,d)` whose rows are samples and columns are features\n",
    "- `y`: the labels of shape `(N,)`\n",
    "- `fit_intercept`: whether to calculate the intercept or not  \n",
    "- `algorithm`: `\"projction\"` or `\"grad_descent\"`\n",
    "- `learning_rate`: learning rate for the gradient descent algorithm\n",
    "- `n_iter`: number of iterations for the gradient descent algorithm \n",
    "\n",
    "**Output:**  \n",
    "A tuple `(predict, coefs, intercep)`.    \n",
    "- `predict`: a function that can takes some samples `X_test` and return the prediction `X_test.dot(coefs) + intercept` \n",
    "- `coef`: an array of shape `(d,)` that stores the coefficients\n",
    "- `intercept`: a float for the intercept\n",
    "\n",
    "**Steps:**\n",
    "1. If `fit_intercept`, let $A$ be the matrix obtained from $X$ by adding a column of ones on the left; otherwise, let $A = X$ (make a copy).  Let `dp` be the number of columns of $A$.\n",
    "2. If `algorithm==\"projection\"`, compute ${\\bf v} = (A^\\top A)^{-1}A^\\top {\\bf y}$.\n",
    "3. If `algorithm==\"grad_descent\"`, run the gradient descent algorithm as follows:\n",
    "    1. Pick a random vector ${\\bf v}$ of shape `(dp,)` .\n",
    "    2. Calculate the gradient $\\nabla = \\frac{2}{N}(A{\\bf v} - y)^\\top A$.\n",
    "    3. Update ${\\bf v}$ by ${\\bf v} - \\alpha\\nabla$.\n",
    "    4. Repeat Steps B and C `n_iter` times.\n",
    "4. If `fit_intercept`, let `coef` be ${\\bf v}[1:]$ and `intercept` be ${\\bf v}[0]$; otherwise, let `coef` be ${\\bf v}$ and `intercept` be 0.  \n",
    "5. Define `predict` as a function that sends `X_test` to `X_test.dot(coefs) + intercept`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode\n",
    "Translate the algorithm into the pseudocode.  \n",
    "This helps you to identify the parts that you don't know how to do it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. \n",
    "    2. \n",
    "    3. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "\n",
    "def LR(X, y, algorithm=\"projection\", learning_rate=.001, n_iter=100, fit_intercept=True):\n",
    "    '''Linear Regression. '''\n",
    "    N = X.shape[0]\n",
    "    A = np.c_[np.ones(N), X] if fit_intercept else X.copy()\n",
    "    \n",
    "    if algorithm==\"projection\":\n",
    "        v = np.linalg.inv(A.T@A)@A.T@y\n",
    "    elif algorithm==\"grad_descent\":\n",
    "        v = np.random.rand(A.shape[1],)\n",
    "        for i in range(n_iter):\n",
    "            d = 2/X.shape[0]*(A@v-y).T@A\n",
    "            v = v - learning_rate*d\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "    (coef, intercept) = (v[1:], v[0]) if fit_intercept else (v, 0)\n",
    "    predict = lambda X_test: X_test@coef + intercept\n",
    "    \n",
    "    return predict, coef, intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "Take some sample data from [LinearRegression-with-scikit-learn](LinearRegression-with-scikit-learn.ipynb) and check if your code generates similar outputs with the existing packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Name of the data\n",
    "Description of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Jephian:\n",
    "You are supposed to give a brief description of the data, e.g., number of samples, features, and how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### results with your code\n",
    "\n",
    "x = np.arange(10)\n",
    "X = x[:, None]\n",
    "y = 0.5 * x + 3 + 0.3*np.random.randn(10)\n",
    "X_test = np.linspace(0,10,20)[:, None]\n",
    "\n",
    "result = LR(X, y)\n",
    "print(\"coef = \", result[1])\n",
    "print(\"intercept = \", result[2])\n",
    "y_new = result[0](X_test)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.scatter(X,y)\n",
    "plt.plot(X_test, y_new,c='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### results with existing packages\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X, y)\n",
    "print(\"coef = \", model.coef_)\n",
    "print(\"intercept = \", model.intercept_)\n",
    "y_new = model.predict(X_test)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.scatter(x,y)\n",
    "plt.plot(X_test,y_new,c = 'r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 1\n",
    "Set `algorithm=\"projection\"` .  \n",
    "Let  \n",
    "```python\n",
    "x = np.arange(10)\n",
    "X1 = np.vstack([x]).T\n",
    "X2 = np.vstack([np.ones_like(x), x]).T\n",
    "y = 0.5 * x + 3 + 0.3*np.random.randn(10)\n",
    "```\n",
    "Apply your code to `X1` with `fit_intercept=True` and obtain `(predict1, coef1, intercept1)` .  \n",
    "Apply your code to `X2` with `fit_intercept=False` and obtain `(predict2, coef2, intercept2)` .  \n",
    "What are the relation between `coef1`, `intercept1` and `coef2` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "X1 = np.vstack([x]).T\n",
    "X2 = np.vstack([np.ones_like(x), x]).T\n",
    "y = 0.5 * x + 3 + 0.3*np.random.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "\n",
    "_, coef1, intercept1 = LR(X1, y)\n",
    "_, coef2, intercept2 = LR(X2, y, fit_intercept=False)\n",
    "print(\"projection:\")\n",
    "print(\"  coef1 = \", coef1)\n",
    "print(\"  intercept1 = \", intercept1)\n",
    "\n",
    "\n",
    "print(\"projection without intercept:\")\n",
    "print(\"  coef2 = \", coef2)\n",
    "print(\"  intercept2 = \", intercept2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "intercept1 = coef2[0]\n",
    "\n",
    "\n",
    "coef1 = coef2[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Jephian:\n",
    "More precisely, it should be `coef1 = coef2[1:]` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 2\n",
    "Let  \n",
    "```python\n",
    "x = np.arange(10)\n",
    "X = np.vstack([x]).T\n",
    "y = 0.5 * x + 3 + 0.3*np.random.randn(10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "X = np.vstack([x]).T\n",
    "y = 0.5 * x + 3 + 0.3*np.random.randn(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2(a)\n",
    "Apply the linear regresssion algorithm to `X`  \n",
    "1. by your code with `algorithm==\"projection\"` ,  \n",
    "2. by your code with `algorithm==\"grad_descent\"` ,  \n",
    "3. by `sklearn.linear_model.LinearRegresssion` .  \n",
    "Check if the outputs are almost the same (up to some numerical errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "\n",
    "_ , coef, intercept = LR(X, y, algorithm=\"projection\")\n",
    "print(\"projection:\")\n",
    "print(\"  coef =\", coef)\n",
    "print(\"  intercept =\", intercept)\n",
    "\n",
    "_ , coef, intercept = LR(X, y, algorithm=\"grad_descent\")\n",
    "print(\"grad_descent:\")\n",
    "print(\"  coef =\", coef)\n",
    "print(\"  intercept =\", intercept)\n",
    "\n",
    "model.fit(X,y)\n",
    "print(\"sklearn.linear_model.LinearRegresssion:\")\n",
    "print(\"  coef =\", model.coef_)\n",
    "print(\"  intercept =\", model.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from projection and sklearn linear regression are the same.\n",
    "\n",
    "But the result from grad_descent is not the same since the result will be affected by the n_iter and learning_rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Jephian:\n",
    "Indeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2(b)\n",
    "Change `learning_rate=0.1` .  \n",
    "What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "\n",
    "_ , coef, intercept = LR(X, y, algorithm=\"grad_descent\", learning_rate=.001)\n",
    "print(\"learning_rate=0.00 :\")\n",
    "print(\"  coef =\", coef)\n",
    "print(\"  intercept =\", intercept)\n",
    "\n",
    "_ , coef, intercept = LR(X, y, algorithm=\"grad_descent\", learning_rate=.1)\n",
    "print(\"learning_rate=0.1:\")\n",
    "print(\"  coef =\", coef)\n",
    "print(\"  intercept =\", intercept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It diverges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2(c)\n",
    "Change `learning_rate=0.0001` .  \n",
    "What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "\n",
    "_ , coef, intercept = LR(X, y, algorithm=\"grad_descent\", learning_rate=.001)\n",
    "print(\"learning_rate=0.001\")\n",
    "print(\"coef = \", coef)\n",
    "print(\"intercept = \", intercept)\n",
    "\n",
    "_ , coef, intercept = LR(X, y, algorithm=\"grad_descent\", learning_rate=.0001)\n",
    "print(\"learning_rate=0.0001\")\n",
    "print(\"coef = \", coef)\n",
    "print(\"intercept = \", intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of grad_descent is not the same as the other two.\n",
    "\n",
    "It doesn't converge well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2(d)\n",
    "Modify your code so that it prints the mean square error at each step of the gradient descent.  \n",
    "Check if it is always decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "\n",
    "def LR_MSE(X, y, algorithm=\"projection\", learning_rate=.001, n_iter=10000, fit_intercept=True):\n",
    "    '''LinearRegression'''\n",
    "    A = np.c_[np.ones(X.shape[0]), X] if fit_intercept else X.copy()\n",
    "    if algorithm==\"projection\":\n",
    "        v = np.linalg.inv(A.T@A)@A.T@y\n",
    "    elif algorithm==\"grad_descent\":\n",
    "        v = np.random.rand(A.shape[1],)\n",
    "        for i in range(n_iter):\n",
    "            d = 2/X.shape[0]*(A@v-y).T@A\n",
    "            v = v - learning_rate*d\n",
    "            print(np.linalg.norm(A.dot(v)-y)**2/X.shape[0])\n",
    "    else:\n",
    "        raise TypeError\n",
    "    (coef, intercept) = (v[1:], v[0]) if fit_intercept else (v, 0)\n",
    "\n",
    "LR_MSE(X, y, algorithm=\"grad_descent\", learning_rate=.001, n_iter=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it's decreasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3\n",
    "This exercise checks if the gradient formula is correct (or at least reasonable).  \n",
    "Let  \n",
    "```python\n",
    "N,dp = 100,3\n",
    "np.random.seed(20025)\n",
    "A = np.random.randn(N,dp)\n",
    "v = np.random.randn(dp)\n",
    "y = np.random.randn(N)\n",
    "```\n",
    "Define $c(A, {\\bf v}, {\\bf y}) = \\frac{1}{N}\\|A{\\bf v} - {\\bf y}\\|^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N,dp = 100,3\n",
    "np.random.seed(20025)\n",
    "A = np.random.randn(N,dp)\n",
    "v = np.random.randn(dp)\n",
    "y = np.random.randn(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3(a)\n",
    "Write a function `cost(A, v, y)` that calculate $c(A, {\\bf v}, {\\bf y})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "\n",
    "def cost(A, v, y):\n",
    "    return 1/N*(np.linalg.norm(A@v-y))**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3(b)\n",
    "Calculate the gradient  \n",
    "$$\\frac{2}{N}(A{\\bf v} - y)^\\top A.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "\n",
    "2/N*(A@v-y).T@A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3(c)\n",
    "Let  \n",
    "```python\n",
    "h = 0.001\n",
    "i = 0\n",
    "e = np.zeros((dp,))\n",
    "e[i] = 1\n",
    "g = (cost(A, v+h*e, y) - cost(A, v, y)) / h\n",
    "```\n",
    "Run the code for `i = 0,1,2` and compare `g` with the gradient in 3(b).  \n",
    "If necessary, change `h` to smaller numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "\n",
    "h = 0.00000001\n",
    "l = []\n",
    "for i in range(3):\n",
    "    e = np.zeros((dp,))\n",
    "    e[i] = 1\n",
    "    g = (cost(A, v+h*e, y) - cost(A, v, y)) / h\n",
    "    l.append(g)\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of 3(b) is similar to the output of 3(c) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR:\n",
    "    '''\n",
    "    Linear Regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : an array of shape (N,d) whose rows are samples and columns are features\n",
    "    y : the labels of shape (N,)\n",
    "    fit_intercept: whether to calculate the intercept or not\n",
    "    algorithm: \"projection\" or \"grad_descent\"\n",
    "    learning_rate: learning rate for the gradient descent algorithm\n",
    "    n_iter: number of iterations for the gradient descent algorithm\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predict: a function that can takes some samples X_test and return the prediction X_test.dot(coefs) + intercept\n",
    "    coef: an array of shape (d,) that stores the coefficients\n",
    "    intercept: a float for the intercept\n",
    "\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        self._algorithm = kwargs.pop('algorithm', \"projection\")\n",
    "        self._a = kwargs.pop('learning_rate', .001)\n",
    "        self._n_iter = kwargs.pop('n_iter', 10000)\n",
    "        self._fit_intercept = kwargs.pop('fit_intercept', True)\n",
    "        if not len(kwargs) == 0:\n",
    "            raise ValueError(f'Unknown keywords ({kwargs.keys()})')\n",
    "        if not self._algorithm in (\"projection\", \"grad_descent\"):\n",
    "            raise ValueError\n",
    "    \n",
    "    def fit(self, X, y, MSQ = False):\n",
    "        N, d = X.shape\n",
    "        A = np.c_[np.ones(N), X] if self._fit_intercept else X.copy()\n",
    "        if self._algorithm==\"projection\":\n",
    "            v = np.linalg.inv(A.T@A)@A.T@y\n",
    "        else:\n",
    "            v = np.random.rand(A.shape[1],)\n",
    "            for i in range(self._n_iter):\n",
    "                d = 2/X.shape[0]*(A@v-y).T@A\n",
    "                v = v - self._a*d\n",
    "                if MSQ == True:\n",
    "                    print(np.linalg.norm(A.dot(v)-y)**2/N)\n",
    "        (self.coef_, self.intercept_) = (v[1:], v[0]) if self._fit_intercept else (v, 0)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return X_test @ self.coef_ + self.intercept_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Jephian:\n",
    "Well done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
