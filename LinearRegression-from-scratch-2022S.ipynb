{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearRegression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Creative Commons License](https://i.creativecommons.org/l/by/4.0/88x31.png)  \n",
    "This work by Jephian Lin is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "**Input:**  \n",
    "- `X`: an array of shape `(N,d)` whose rows are samples and columns are features\n",
    "- `y`: the labels of shape `(N,)`\n",
    "- `fit_intercept`: whether to calculate the intercept or not  \n",
    "- `algorithm`: `\"projction\"` or `\"grad_descent\"`\n",
    "- `learning_rate`: learning rate for the gradient descent algorithm\n",
    "- `n_iter`: number of iterations for the gradient descent algorithm \n",
    "\n",
    "**Output:**  \n",
    "A tuple `(predict, coefs, intercep)`.    \n",
    "- `predict`: a function that can takes some samples `X_test` and return the prediction `X_test.dot(coefs) + intercept` \n",
    "- `coef`: an array of shape `(d,)` that stores the coefficients\n",
    "- `intercept`: a float for the intercept\n",
    "\n",
    "**Steps:**\n",
    "1. If `fit_intercept`, let $A$ be the matrix obtained from $X$ by adding a column of ones on the left; otherwise, let $A = X$ (make a copy).  Let `dp` be the number of columns of $A$.\n",
    "2. If `algorithm==\"projection\"`, compute ${\\bf v} = (A^\\top A)^{-1}A^\\top {\\bf y}$.\n",
    "3. If `algorithm==\"grad_descent\"`, run the gradient descent algorithm as follows:\n",
    "    1. Pick a random vector ${\\bf v}$ of shape `(dp,)` .\n",
    "    2. Calculate the gradient $\\nabla = \\frac{2}{N}(A{\\bf v} - {\\bf y})^\\top A$.\n",
    "    3. Update ${\\bf v}$ by ${\\bf v} - \\alpha\\nabla$.\n",
    "    4. Repeat Steps B and C `n_iter` times.\n",
    "4. If `fit_intercept`, let `coef` be ${\\bf v}[1:]$ and `intercept` be ${\\bf v}[0]$; otherwise, let `coef` be ${\\bf v}$ and `intercept` be 0.  \n",
    "5. Define `predict` as a function that sends `X_test` to `X_test.dot(coefs) + intercept`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode\n",
    "Translate the algorithm into the pseudocode.  \n",
    "This helps you to identify the parts that you don't know how to do it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. \n",
    "    2. \n",
    "    3. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "\n",
    "def LR(X, y, fit_intercept=True, algorithm=\"projection\", learning_rate=0.01, n_iter=1000):\n",
    "    \n",
    "    N,d = X.shape\n",
    "    \n",
    "    if fit_intercept:\n",
    "        A = np.hstack([np.ones((N,1)),X])\n",
    "        dp = d+1\n",
    "    else:\n",
    "        A = X.copy()\n",
    "        dp = d\n",
    "        \n",
    "    if algorithm == \"projection\":\n",
    "        v = np.linalg.inv(A.T.dot(A)).dot(A.T).dot(y)\n",
    "    elif algorithm == \"grad_descent\":\n",
    "        v = np.random.rand(dp,)\n",
    "        for i in range(n_iter):\n",
    "            grad = (2/N)*((A.dot(v)-y).T).dot(A)\n",
    "            v = v - learning_rate*grad\n",
    "                \n",
    "    if fit_intercept:\n",
    "        coef = v[1:]\n",
    "        intercept = v[0]\n",
    "    else:\n",
    "        coef = v.copy()\n",
    "        intercept = 0\n",
    "        \n",
    "    predict = lambda X_test: X_test.dot(coef) + intercept\n",
    "        \n",
    "    return predict, coef, intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "Take some sample data from [LinearRegression-with-scikit-learn](LinearRegression-with-scikit-learn.ipynb) and check if your code generates similar outputs with the existing packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Name of the data\n",
    "Description of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alex:\n",
    "You should give a brief description of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### results with your code\n",
    "### (sklearn exercise 1)\n",
    "x = np.arange(10)\n",
    "y = 0.5*x + 3 + 0.3*np.random.randn(10)\n",
    "X = x[:,np.newaxis]\n",
    "x_test = np.linspace(0,10,20)\n",
    "X_test = x_test[:,np.newaxis]\n",
    "\n",
    "predict, coef, intercept = LR(X, y, algorithm=\"projection\")\n",
    "y_new = predict(X_test)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x_test, y_new, c='r')\n",
    "\n",
    "print('coef_=', coef)\n",
    "print('intercept_=', intercept) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### results with existing packages\n",
    "x = np.arange(10)\n",
    "y = 0.5*x + 3 + 0.3*np.random.randn(10)\n",
    "X = x[:,np.newaxis]\n",
    "x_test = np.linspace(0,10,20)\n",
    "X_test = x_test[:,np.newaxis]\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_new = model.predict(X_test)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x_test, y_new, c='r')\n",
    "\n",
    "print('model.coef_=', model.coef_)   \n",
    "print('model.intercept_=', model.intercept_)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alex:\n",
    "Since you randomize your data two times, the data for using package and your own function are not the same. Thus, you get a different result. You should use the same data to check whether your code generates similar outputs with the existing packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 1\n",
    "Set `algorithm=\"projection\"` .  \n",
    "Let  \n",
    "```python\n",
    "x = np.arange(10)\n",
    "X1 = np.vstack([x]).T\n",
    "X2 = np.vstack([np.ones_like(x), x]).T\n",
    "y = 0.5 * x + 3 + 0.3*np.random.randn(10)\n",
    "```\n",
    "Apply your code to `X1` with `fit_intercept=True` and obtain `(predict1, coef1, intercept1)` .  \n",
    "Apply your code to `X2` with `fit_intercept=False` and obtain `(predict2, coef2, intercept2)` .  \n",
    "What are the relation between `coef1`, `intercept1` and `coef2` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "\n",
    "x = np.arange(10)\n",
    "X1 = np.vstack([x]).T\n",
    "X2 = np.vstack([np.ones_like(x), x]).T\n",
    "y = 0.5 * x + 3 + 0.3*np.random.randn(10)\n",
    "\n",
    "predict1, coef1, intercept1 = LR(X1, y, algorithm=\"projection\")\n",
    "print('coef1 = ',coef1)\n",
    "print('intercept1 = ',intercept1)\n",
    "\n",
    "predict2, coef2, intercept2 = LR(X2, y, algorithm=\"projection\", fit_intercept=False)\n",
    "print('coef2 = ',coef2)\n",
    "\n",
    "\n",
    "# coef1 = coef2[1:]\n",
    "# intercept1 = coef2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 2\n",
    "Let  \n",
    "```python\n",
    "x = np.arange(10)\n",
    "X = np.vstack([x]).T\n",
    "y = 0.5 * x + 3 + 0.3*np.random.randn(10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2(a)\n",
    "Apply the linear regresssion algorithm to `X`  \n",
    "1. by your code with `algorithm==\"projection\"` ,  \n",
    "2. by your code with `algorithm==\"grad_descent\"` ,  \n",
    "3. by `sklearn.linear_model.LinearRegresssion` .  \n",
    "Check if the outputs are almost the same (up to some numerical errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "X = np.vstack([x]).T\n",
    "y = 0.5 * x + 3 + 0.3*np.random.randn(10)\n",
    "\n",
    "predict, coef, intercept = LR(X, y, algorithm=\"projection\")\n",
    "print('projection')\n",
    "print('coef = ',coef)\n",
    "print('intercept = ',intercept)\n",
    "\n",
    "predict, coef, intercept = LR(X, y, algorithm=\"grad_descent\")\n",
    "print('grad_descent')\n",
    "print('coef = ',coef)\n",
    "print('intercept = ',intercept)\n",
    " \n",
    "model = LinearRegression()\n",
    "model.fit(X,y)\n",
    "print('sklearn.linear_model.LinearRegression')\n",
    "print('coef = ',model.coef_)\n",
    "print('intercept = ',model.intercept_)\n",
    "\n",
    "# The results from projestion and sklearn LR are almost the same. \n",
    "# The result from gradient descent is different from the 2nd numerical error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2(b)\n",
    "Change `learning_rate=0.1` .  \n",
    "What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "\n",
    "predict, coef, intercept = LR(X, y, algorithm=\"grad_descent\", learning_rate=0.1)\n",
    "\n",
    "print(' Change learning_rate=0.1: ')\n",
    "print(coef)\n",
    "print(intercept)\n",
    "\n",
    "# It diverges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2(c)\n",
    "Change `learning_rate=0.0001` .  \n",
    "What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "\n",
    "predict, coef, intercept = LR(X, y, algorithm=\"grad_descent\", learning_rate=0.0001)\n",
    "\n",
    "print(' Change learning_rate=0.0001: ')\n",
    "print(coef)\n",
    "print(intercept)\n",
    "\n",
    "# It did not converge well since the learning rate is too small.\n",
    "# The result is different to algorithm='projection' and sklearn.\n",
    "# The result from grad_descent for learning_rate=0.0001 with the result from grad_descent for learning_rate=0.01 are not the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2(d)\n",
    "Modify your code so that it prints the mean square error at each step of the gradient descent.  \n",
    "Check if it is always decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "\n",
    "def LR_gradient(X, y, fit_intercept=True, algorithm=\"projction\", learning_rate = 0.01, n_iter = 10):\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    \n",
    "    if fit_intercept:\n",
    "        A = np.hstack([np.ones((N,1)),X])\n",
    "        dp = d+1\n",
    "    else:\n",
    "        A = X.copy()\n",
    "        dp = d\n",
    "        \n",
    "    if algorithm == \"projection\":\n",
    "        v = np.linalg.inv(A.T.dot(A)).dot(A.T).dot(y)\n",
    "    elif algorithm == \"grad_descent\":\n",
    "        v = np.random.rand(dp,)\n",
    "        for i in range(n_iter):\n",
    "            grad = (2/N)*((A.dot(v)-y).T).dot(A)\n",
    "            v = v - learning_rate*grad\n",
    "            print(np.linalg.norm(A.dot(v)-y)**2/X.shape[0])\n",
    "            \n",
    "LR_gradient(X, y, algorithm = \"grad_descent\")\n",
    "\n",
    "# Yes, it is always decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3\n",
    "This exercise checks if the gradient formula is correct (or at least reasonable).  \n",
    "Let  \n",
    "```python\n",
    "N,dp = 100,3\n",
    "np.random.seed(20025)\n",
    "A = np.random.randn(N,dp)\n",
    "v = np.random.randn(dp)\n",
    "y = np.random.randn(N)\n",
    "```\n",
    "Define $c(A, {\\bf v}, {\\bf y}) = \\frac{1}{N}\\|A{\\bf v} - {\\bf y}\\|^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3(a)\n",
    "Write a function `cost(A, v, y)` that calculate $c(A, {\\bf v}, {\\bf y})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "\n",
    "N,dp = 100,3\n",
    "np.random.seed(20025)\n",
    "A = np.random.randn(N,dp)\n",
    "v = np.random.randn(dp)\n",
    "y = np.random.randn(N)\n",
    "\n",
    "def cost(A, v, y):\n",
    "    return (1/N)*(np.linalg.norm(A.dot(v)-y))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3(b)\n",
    "Calculate the gradient  \n",
    "$$\\frac{2}{N}(A{\\bf v} - y)^\\top A.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "\n",
    "2/N*(A.dot(v)-y).T.dot(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3(c)\n",
    "Let  \n",
    "```python\n",
    "h = 0.001\n",
    "i = 0\n",
    "e = np.zeros((dp,))\n",
    "e[i] = 1\n",
    "g = (cost(A, v+h*e, y) - cost(A, v, y)) / h\n",
    "```\n",
    "Run the code for `i = 0,1,2` and compare `g` with the gradient in 3(b).  \n",
    "If necessary, change `h` to smaller numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your answer here\n",
    "\n",
    "h = 0.00000001\n",
    "i = 0\n",
    "\n",
    "for i in range(3):\n",
    "    e = np.zeros((dp,))\n",
    "    e[i] = 1\n",
    "    g = (cost(A, v+h*e, y) - cost(A, v, y)) / h\n",
    "    print(g)\n",
    "    \n",
    "# g is similar to the gradient in 3(b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
